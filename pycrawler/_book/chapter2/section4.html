<!DOCTYPE HTML>
<html lang="en" >
    
    <head>
        
        <meta charset="UTF-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge" />
        <title>4. Python爬虫实战四之抓取淘宝MM照片 | Python爬虫学习系列教程</title>
        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <meta name="description" content="">
        <meta name="generator" content="GitBook 2.6.7">
        
        
        <meta name="HandheldFriendly" content="true"/>
        <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
        <meta name="apple-mobile-web-app-capable" content="yes">
        <meta name="apple-mobile-web-app-status-bar-style" content="black">
        <link rel="apple-touch-icon-precomposed" sizes="152x152" href="../gitbook/images/apple-touch-icon-precomposed-152.png">
        <link rel="shortcut icon" href="../gitbook/images/favicon.ico" type="image/x-icon">
        
    <link rel="stylesheet" href="../gitbook/style.css">
    
        
        <link rel="stylesheet" href="../gitbook/plugins/gitbook-plugin-highlight/website.css">
        
    
        
        <link rel="stylesheet" href="../gitbook/plugins/gitbook-plugin-search/search.css">
        
    
        
        <link rel="stylesheet" href="../gitbook/plugins/gitbook-plugin-fontsettings/website.css">
        
    
    

        
    
    
    <link rel="next" href="../chapter2/section5.html" />
    
    
    <link rel="prev" href="../chapter2/section3.html" />
    

        
    </head>
    <body>
        
        
    <div class="book"
        data-level="2.4"
        data-chapter-title="4. Python爬虫实战四之抓取淘宝MM照片"
        data-filepath="chapter2/section4.md"
        data-basepath=".."
        data-revision="Wed Jun 21 2017 19:16:14 GMT+0800 (中国标准时间)"
        data-innerlanguage="">
    

<div class="book-summary">
    <nav role="navigation">
        <ul class="summary">
            
            
            
            

            

            
    
        <li class="chapter " data-level="0" data-path="index.html">
            
                
                    <a href="../index.html">
                
                        <i class="fa fa-check"></i>
                        
                        Python爬虫学习系列教程
                    </a>
            
            
        </li>
    
        <li class="chapter " data-level="1" data-path="chapter1/index.html">
            
                
                    <a href="../chapter1/index.html">
                
                        <i class="fa fa-check"></i>
                        
                            <b>1.</b>
                        
                        一、爬虫入门
                    </a>
            
            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.1" data-path="chapter1/section1.html">
            
                
                    <a href="../chapter1/section1.html">
                
                        <i class="fa fa-check"></i>
                        
                            <b>1.1.</b>
                        
                        1. Python爬虫入门一之综述
                    </a>
            
            
        </li>
    
        <li class="chapter " data-level="1.2" data-path="chapter1/section2.html">
            
                
                    <a href="../chapter1/section2.html">
                
                        <i class="fa fa-check"></i>
                        
                            <b>1.2.</b>
                        
                        2. Python爬虫入门二之爬虫基础了解
                    </a>
            
            
        </li>
    
        <li class="chapter " data-level="1.3" data-path="chapter1/section3.html">
            
                
                    <a href="../chapter1/section3.html">
                
                        <i class="fa fa-check"></i>
                        
                            <b>1.3.</b>
                        
                        3. Python爬虫入门三之Urllib库的基本使用
                    </a>
            
            
        </li>
    
        <li class="chapter " data-level="1.4" data-path="chapter1/section4.html">
            
                
                    <a href="../chapter1/section4.html">
                
                        <i class="fa fa-check"></i>
                        
                            <b>1.4.</b>
                        
                        4. Python爬虫入门四之Urllib库的高级用法
                    </a>
            
            
        </li>
    
        <li class="chapter " data-level="1.5" data-path="chapter1/section5.html">
            
                
                    <a href="../chapter1/section5.html">
                
                        <i class="fa fa-check"></i>
                        
                            <b>1.5.</b>
                        
                        5. Python爬虫入门五之URLError异常处理
                    </a>
            
            
        </li>
    
        <li class="chapter " data-level="1.6" data-path="chapter1/section6.html">
            
                
                    <a href="../chapter1/section6.html">
                
                        <i class="fa fa-check"></i>
                        
                            <b>1.6.</b>
                        
                        6. Python爬虫入门六之Cookie的使用
                    </a>
            
            
        </li>
    
        <li class="chapter " data-level="1.7" data-path="chapter1/section7.html">
            
                
                    <a href="../chapter1/section7.html">
                
                        <i class="fa fa-check"></i>
                        
                            <b>1.7.</b>
                        
                        7. Python爬虫入门七之正则表达式
                    </a>
            
            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="2" data-path="chapter2/index.html">
            
                
                    <a href="../chapter2/index.html">
                
                        <i class="fa fa-check"></i>
                        
                            <b>2.</b>
                        
                        二、爬虫实战
                    </a>
            
            
            <ul class="articles">
                
    
        <li class="chapter " data-level="2.1" data-path="chapter2/section1.html">
            
                
                    <a href="../chapter2/section1.html">
                
                        <i class="fa fa-check"></i>
                        
                            <b>2.1.</b>
                        
                        1. Python爬虫实战一之爬取糗事百科段子
                    </a>
            
            
        </li>
    
        <li class="chapter " data-level="2.2" data-path="chapter2/section2.html">
            
                
                    <a href="../chapter2/section2.html">
                
                        <i class="fa fa-check"></i>
                        
                            <b>2.2.</b>
                        
                        2. Python爬虫实战二之爬取百度贴吧帖子
                    </a>
            
            
        </li>
    
        <li class="chapter " data-level="2.3" data-path="chapter2/section3.html">
            
                
                    <a href="../chapter2/section3.html">
                
                        <i class="fa fa-check"></i>
                        
                            <b>2.3.</b>
                        
                        3. Python爬虫实战三之实现山东大学无线网络掉线自动重连
                    </a>
            
            
        </li>
    
        <li class="chapter active" data-level="2.4" data-path="chapter2/section4.html">
            
                
                    <a href="../chapter2/section4.html">
                
                        <i class="fa fa-check"></i>
                        
                            <b>2.4.</b>
                        
                        4. Python爬虫实战四之抓取淘宝MM照片
                    </a>
            
            
        </li>
    
        <li class="chapter " data-level="2.5" data-path="chapter2/section5.html">
            
                
                    <a href="../chapter2/section5.html">
                
                        <i class="fa fa-check"></i>
                        
                            <b>2.5.</b>
                        
                        5. Python爬虫实战五之模拟登录淘宝并获取所有订单
                    </a>
            
            
        </li>
    
        <li class="chapter " data-level="2.6" data-path="chapter2/section6.html">
            
                
                    <a href="../chapter2/section6.html">
                
                        <i class="fa fa-check"></i>
                        
                            <b>2.6.</b>
                        
                        6. Python爬虫实战六之抓取爱问知识人问题并保存至数据库
                    </a>
            
            
        </li>
    
        <li class="chapter " data-level="2.7" data-path="chapter2/section7.html">
            
                
                    <a href="../chapter2/section7.html">
                
                        <i class="fa fa-check"></i>
                        
                            <b>2.7.</b>
                        
                        7. Python爬虫实战七之计算大学本学期绩点
                    </a>
            
            
        </li>
    
        <li class="chapter " data-level="2.8" data-path="chapter2/section8.html">
            
                
                    <a href="../chapter2/section8.html">
                
                        <i class="fa fa-check"></i>
                        
                            <b>2.8.</b>
                        
                        8. Python爬虫实战八之利用Selenium抓取淘宝匿名旺旺
                    </a>
            
            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="3" data-path="chapter3/index.html">
            
                
                    <a href="../chapter3/index.html">
                
                        <i class="fa fa-check"></i>
                        
                            <b>3.</b>
                        
                        三、爬虫利器
                    </a>
            
            
            <ul class="articles">
                
    
        <li class="chapter " data-level="3.1" data-path="chapter3/section1.html">
            
                
                    <a href="../chapter3/section1.html">
                
                        <i class="fa fa-check"></i>
                        
                            <b>3.1.</b>
                        
                        1. Python爬虫利器一之Requests库的用法
                    </a>
            
            
        </li>
    
        <li class="chapter " data-level="3.2" data-path="chapter3/section2.html">
            
                
                    <a href="../chapter3/section2.html">
                
                        <i class="fa fa-check"></i>
                        
                            <b>3.2.</b>
                        
                        2. Python爬虫利器二之Beautiful Soup的用法
                    </a>
            
            
        </li>
    
        <li class="chapter " data-level="3.3" data-path="chapter3/section3.html">
            
                
                    <a href="../chapter3/section3.html">
                
                        <i class="fa fa-check"></i>
                        
                            <b>3.3.</b>
                        
                        3. Python爬虫利器三之Xpath语法与lxml库的用法
                    </a>
            
            
        </li>
    
        <li class="chapter " data-level="3.4" data-path="chapter3/section4.html">
            
                
                    <a href="../chapter3/section4.html">
                
                        <i class="fa fa-check"></i>
                        
                            <b>3.4.</b>
                        
                        4. Python爬虫利器四之PhantomJS的用法
                    </a>
            
            
        </li>
    
        <li class="chapter " data-level="3.5" data-path="chapter3/section5.html">
            
                
                    <a href="../chapter3/section5.html">
                
                        <i class="fa fa-check"></i>
                        
                            <b>3.5.</b>
                        
                        5. Python爬虫利器五之Selenium的用法
                    </a>
            
            
        </li>
    
        <li class="chapter " data-level="3.6" data-path="chapter3/section6.html">
            
                
                    <a href="../chapter3/section6.html">
                
                        <i class="fa fa-check"></i>
                        
                            <b>3.6.</b>
                        
                        6. Python爬虫利器六之PyQuery的用法
                    </a>
            
            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="4" data-path="chapter4/index.html">
            
                
                    <a href="../chapter4/index.html">
                
                        <i class="fa fa-check"></i>
                        
                            <b>4.</b>
                        
                        四、爬虫进阶
                    </a>
            
            
            <ul class="articles">
                
    
        <li class="chapter " data-level="4.1" data-path="chapter4/section1.html">
            
                
                    <a href="../chapter4/section1.html">
                
                        <i class="fa fa-check"></i>
                        
                            <b>4.1.</b>
                        
                        1. Python爬虫进阶一之爬虫框架概述
                    </a>
            
            
        </li>
    
        <li class="chapter " data-level="4.2" data-path="chapter4/section2.html">
            
                
                    <a href="../chapter4/section2.html">
                
                        <i class="fa fa-check"></i>
                        
                            <b>4.2.</b>
                        
                        2. Python爬虫进阶二之PySpider框架安装配置
                    </a>
            
            
        </li>
    
        <li class="chapter " data-level="4.3" data-path="chapter4/section3.html">
            
                
                    <a href="../chapter4/section3.html">
                
                        <i class="fa fa-check"></i>
                        
                            <b>4.3.</b>
                        
                        3. Python爬虫进阶三之爬虫框架Scrapy安装配置
                    </a>
            
            
        </li>
    
        <li class="chapter " data-level="4.4" data-path="chapter4/section4.html">
            
                
                    <a href="../chapter4/section4.html">
                
                        <i class="fa fa-check"></i>
                        
                            <b>4.4.</b>
                        
                        4. Python爬虫进阶四之PySpider的用法
                    </a>
            
            
        </li>
    
        <li class="chapter " data-level="4.5" data-path="chapter4/section5.html">
            
                
                    <a href="../chapter4/section5.html">
                
                        <i class="fa fa-check"></i>
                        
                            <b>4.5.</b>
                        
                        5. Python爬虫进阶五之多线程的用法
                    </a>
            
            
        </li>
    
        <li class="chapter " data-level="4.6" data-path="chapter4/section6.html">
            
                
                    <a href="../chapter4/section6.html">
                
                        <i class="fa fa-check"></i>
                        
                            <b>4.6.</b>
                        
                        6. Python爬虫进阶六之多进程的用法
                    </a>
            
            
        </li>
    
        <li class="chapter " data-level="4.7" data-path="chapter4/section7.html">
            
                
                    <a href="../chapter4/section7.html">
                
                        <i class="fa fa-check"></i>
                        
                            <b>4.7.</b>
                        
                        7. Python爬虫进阶七之设置ADSL拨号服务器代理
                    </a>
            
            
        </li>
    

            </ul>
            
        </li>
    


            
            <li class="divider"></li>
            <li>
                <a href="https://www.gitbook.com" target="blank" class="gitbook-link">
                    Published with GitBook
                </a>
            </li>
            
        </ul>
    </nav>
</div>

    <div class="book-body">
        <div class="body-inner">
            <div class="book-header" role="navigation">
    <!-- Actions Left -->
    

    <!-- Title -->
    <h1>
        <i class="fa fa-circle-o-notch fa-spin"></i>
        <a href="../" >Python爬虫学习系列教程</a>
    </h1>
</div>

            <div class="page-wrapper" tabindex="-1" role="main">
                <div class="page-inner">
                
                
                    <section class="normal" id="section-">
                    
                        <h1 id="python&#x722C;&#x866B;&#x5B9E;&#x6218;&#x56DB;&#x4E4B;&#x6293;&#x53D6;&#x6DD8;&#x5B9D;mm&#x7167;&#x7247;">Python&#x722C;&#x866B;&#x5B9E;&#x6218;&#x56DB;&#x4E4B;&#x6293;&#x53D6;&#x6DD8;&#x5B9D;MM&#x7167;&#x7247;</h1>
<p>&#x798F;&#x5229;&#x554A;&#x798F;&#x5229;&#xFF0C;&#x672C;&#x6B21;&#x4E3A;&#x5927;&#x5BB6;&#x5E26;&#x6765;&#x7684;&#x9879;&#x76EE;&#x662F;&#x6293;&#x53D6;&#x6DD8;&#x5B9D;MM&#x7167;&#x7247;&#x5E76;&#x4FDD;&#x5B58;&#x8D77;&#x6765;&#xFF0C;&#x5927;&#x5BB6;&#x6709;&#x6CA1;&#x6709;&#x5F88;&#x6FC0;&#x52A8;&#x5462;&#xFF1F;</p>
<h2 id="&#x6700;&#x65B0;&#x52A8;&#x6001;">&#x6700;&#x65B0;&#x52A8;&#x6001;</h2>
<blockquote>
<p>&#x66F4;&#x65B0;&#x65F6;&#x95F4;&#xFF1A;2015/8/2
&#x6700;&#x8FD1;&#x597D;&#x591A;&#x8BFB;&#x8005;&#x53CD;&#x6620;&#x4EE3;&#x7801;&#x5DF2;&#x7ECF;&#x4E0D;&#x80FD;&#x7528;&#x4E86;&#xFF0C;&#x539F;&#x56E0;&#x662F;&#x6DD8;&#x5B9D;&#x7D22;&#x5F15;&#x9875;&#x7684;MM&#x94FE;&#x63A5;&#x6539;&#x4E86;&#x3002;&#x7F51;&#x7AD9;&#x6539;&#x7248;&#x4E86;&#xFF0C;URL&#x7684;&#x7D22;&#x5F15;&#x5DF2;&#x7ECF;&#x548C;&#x4E4B;&#x524D;&#x7684;&#x4E0D;&#x4E00;&#x6837;&#x4E86;&#xFF0C;&#x4E4B;&#x524D;&#x53EF;&#x4EE5;&#x76F4;&#x63A5;&#x8DF3;&#x8F6C;&#x5230;&#x6BCF;&#x4E2A;MM&#x7684;&#x4E2A;&#x6027;&#x57DF;&#x540D;&#xFF0C;&#x73B0;&#x5728;&#x4E2D;&#x95F4;&#x52A0;&#x4E86;&#x4E00;&#x4E2A;&#x8DF3;&#x8F6C;&#x9875;&#xFF0C;&#x672C;&#x4EE5;&#x4E3A;&#x53EF;&#x4EE5;&#x901A;&#x8FC7;&#x8FD9;&#x4E2A;&#x9875;&#x9762;&#x7136;&#x540E;&#x8DF3;&#x8F6C;&#x5230;&#x539F;&#x6765;&#x7684;&#x4E2A;&#x6027;&#x57DF;&#x540D;&#xFF0C;&#x800C;&#x7ECF;&#x8FC7;&#x4E00;&#x756A;&#x6298;&#x817E;&#x53D1;&#x73B0;&#xFF0C;&#x8FD9;&#x4E2A;&#x8DF3;&#x8F6C;&#x9875;&#x4E2D;&#x7684;&#x5185;&#x5BB9;&#x662F;JS&#x52A8;&#x6001;&#x751F;&#x6210;&#x7684;&#xFF0C;&#x6240;&#x4EE5;&#x4E0D;&#x80FD;&#x7528;Urllib&#x5E93;&#x6765;&#x76F4;&#x63A5;&#x6293;&#x53D6;&#x4E86;&#xFF0C;&#x672C;&#x7BC7;&#x5C31;&#x53EA;&#x63D0;&#x4F9B;&#x5B66;&#x4E60;&#x601D;&#x8DEF;&#xFF0C;&#x4EE3;&#x7801;&#x4E0D;&#x80FD;&#x7EE7;&#x7EED;&#x7528;&#x4E86;&#x3002;
&#x4E4B;&#x540E;&#x535A;&#x4E3B;&#x4F1A;&#x5229;&#x7528;&#x5176;&#x5B83;&#x65B9;&#x6CD5;&#x6765;&#x5C1D;&#x8BD5;&#x89E3;&#x51B3;&#xFF0C;&#x5982;&#x679C;&#x89E3;&#x51B3;&#xFF0C;&#x7B2C;&#x4E00;&#x65F6;&#x95F4;&#x66F4;&#x65B0;&#xFF01;&#x8C22;&#x8C22;&#x5927;&#x5BB6;&#xFF01;</p>
<p>&#x66F4;&#x65B0;&#x65F6;&#x95F4;&#xFF1A;2016/3/26
&#x5982;&#x4E0A;&#x95EE;&#x9898;&#x5DF2;&#x89E3;&#x51B3;&#xFF0C;&#x5229;&#x7528; PhantomJS&#x7684;&#x52A8;&#x6001;&#x89E3;&#x6790;&#x5373;&#x53EF;&#x5B8C;&#x6210;&#x3002;&#x56E0;&#x4E3A; PySpider &#x540C;&#x6837;&#x652F;&#x6301; PhantomJS&#xFF0C;&#x6240;&#x4EE5;&#x6211;&#x76F4;&#x63A5;&#x5229;&#x7528;&#x4E86; PySpider &#x6765;&#x5B8C;&#x6210;&#xFF0C;&#x89E3;&#x51B3;&#x65B9;&#x6848;&#x5982;&#x4E0B;
<a href="http://cuiqingcai.com/2652.html" target="_blank">&#x89E3;&#x51B3;&#x65B9;&#x6848;</a>
&#x53E6;&#x5916;&#x5982;&#x679C;&#x4E0D;&#x60F3;&#x4F7F;&#x7528;&#x6846;&#x67B6;&#xFF0C;&#x53EF;&#x4EE5;&#x76F4;&#x63A5;&#x5229;&#x7528; Selenium + PhantomJS &#x6765;&#x89E3;&#x6790;&#xFF0C;&#x540C;&#x6837;&#x65B9;&#x4FBF;&#xFF0C;&#x89E3;&#x51B3;&#x65B9;&#x6848;&#x53EF;&#x4EE5;&#x53C2;&#x8003;
<a href="http://cuiqingcai.com/2599.html" target="_blank">&#x52A8;&#x6001;&#x89E3;&#x6790;&#x89E3;&#x51B3;&#x65B9;&#x6848;</a></p>
</blockquote>
<h2 id="&#x672C;&#x7BC7;&#x76EE;&#x6807;">&#x672C;&#x7BC7;&#x76EE;&#x6807;</h2>
<p>1.&#x6293;&#x53D6;&#x6DD8;&#x5B9D;MM&#x7684;&#x59D3;&#x540D;&#xFF0C;&#x5934;&#x50CF;&#xFF0C;&#x5E74;&#x9F84;</p>
<p>2.&#x6293;&#x53D6;&#x6BCF;&#x4E00;&#x4E2A;MM&#x7684;&#x8D44;&#x6599;&#x7B80;&#x4ECB;&#x4EE5;&#x53CA;&#x5199;&#x771F;&#x56FE;&#x7247;</p>
<p>3.&#x628A;&#x6BCF;&#x4E00;&#x4E2A;MM&#x7684;&#x5199;&#x771F;&#x56FE;&#x7247;&#x6309;&#x7167;&#x6587;&#x4EF6;&#x5939;&#x4FDD;&#x5B58;&#x5230;&#x672C;&#x5730;</p>
<p>4.&#x719F;&#x6089;&#x6587;&#x4EF6;&#x4FDD;&#x5B58;&#x7684;&#x8FC7;&#x7A0B;</p>
<h2 id="1url&#x7684;&#x683C;&#x5F0F;">1.URL&#x7684;&#x683C;&#x5F0F;</h2>
<p>&#x5728;&#x8FD9;&#x91CC;&#x6211;&#x4EEC;&#x7528;&#x5230;&#x7684;URL&#x662F; <a href="http://mm.taobao.com/json/request_top_list.htm?page=1&#xFF0C;&#x95EE;&#x53F7;&#x524D;&#x9762;&#x662F;&#x57FA;&#x5730;&#x5740;&#xFF0C;&#x540E;&#x9762;&#x7684;&#x53C2;&#x6570;page&#x662F;&#x4EE3;&#x8868;&#x7B2C;&#x51E0;&#x9875;&#xFF0C;&#x53EF;&#x4EE5;&#x968F;&#x610F;&#x66F4;&#x6362;&#x5730;&#x5740;&#x3002;&#x70B9;&#x51FB;&#x5F00;&#x4E4B;&#x540E;&#xFF0C;&#x4F1A;&#x53D1;&#x73B0;&#x6709;&#x4E00;&#x4E9B;&#x6DD8;&#x5B9D;MM&#x7684;&#x7B80;&#x4ECB;&#xFF0C;&#x5E76;&#x9644;&#x6709;&#x8D85;&#x94FE;&#x63A5;&#x94FE;&#x63A5;&#x5230;&#x4E2A;&#x4EBA;&#x8BE6;&#x60C5;&#x9875;&#x9762;&#x3002;" target="_blank">http://mm.taobao.com/json/request_top_list.htm?page=1&#xFF0C;&#x95EE;&#x53F7;&#x524D;&#x9762;&#x662F;&#x57FA;&#x5730;&#x5740;&#xFF0C;&#x540E;&#x9762;&#x7684;&#x53C2;&#x6570;page&#x662F;&#x4EE3;&#x8868;&#x7B2C;&#x51E0;&#x9875;&#xFF0C;&#x53EF;&#x4EE5;&#x968F;&#x610F;&#x66F4;&#x6362;&#x5730;&#x5740;&#x3002;&#x70B9;&#x51FB;&#x5F00;&#x4E4B;&#x540E;&#xFF0C;&#x4F1A;&#x53D1;&#x73B0;&#x6709;&#x4E00;&#x4E9B;&#x6DD8;&#x5B9D;MM&#x7684;&#x7B80;&#x4ECB;&#xFF0C;&#x5E76;&#x9644;&#x6709;&#x8D85;&#x94FE;&#x63A5;&#x94FE;&#x63A5;&#x5230;&#x4E2A;&#x4EBA;&#x8BE6;&#x60C5;&#x9875;&#x9762;&#x3002;</a></p>
<p>&#x6211;&#x4EEC;&#x9700;&#x8981;&#x6293;&#x53D6;&#x672C;&#x9875;&#x9762;&#x7684;&#x5934;&#x50CF;&#x5730;&#x5740;&#xFF0C;MM&#x59D3;&#x540D;&#xFF0C;MM&#x5E74;&#x9F84;&#xFF0C;MM&#x5C45;&#x4F4F;&#x5730;&#xFF0C;&#x4EE5;&#x53CA;MM&#x7684;&#x4E2A;&#x4EBA;&#x8BE6;&#x60C5;&#x9875;&#x9762;&#x5730;&#x5740;&#x3002;</p>
<h2 id="2&#x6293;&#x53D6;&#x7B80;&#x8981;&#x4FE1;&#x606F;">2.&#x6293;&#x53D6;&#x7B80;&#x8981;&#x4FE1;&#x606F;</h2>
<p>&#x76F8;&#x4FE1;&#x5927;&#x5BB6;&#x7ECF;&#x8FC7;&#x4E0A;&#x51E0;&#x6B21;&#x7684;&#x5B9E;&#x6218;&#xFF0C;&#x5BF9;&#x6293;&#x53D6;&#x548C;&#x63D0;&#x53D6;&#x9875;&#x9762;&#x7684;&#x5730;&#x5740;&#x5DF2;&#x7ECF;&#x975E;&#x5E38;&#x719F;&#x6089;&#x4E86;&#xFF0C;&#x8FD9;&#x91CC;&#x6CA1;&#x6709;&#x4EC0;&#x4E48;&#x96BE;&#x5EA6;&#x4E86;&#xFF0C;&#x6211;&#x4EEC;&#x9996;&#x5148;&#x6293;&#x53D6;&#x672C;&#x9875;&#x9762;&#x7684;MM&#x8BE6;&#x60C5;&#x9875;&#x9762;&#x5730;&#x5740;&#xFF0C;&#x59D3;&#x540D;&#xFF0C;&#x5E74;&#x9F84;&#x7B49;&#x7B49;&#x7684;&#x4FE1;&#x606F;&#x6253;&#x5370;&#x51FA;&#x6765;&#xFF0C;&#x76F4;&#x63A5;&#x8D34;&#x4EE3;&#x7801;&#x5982;&#x4E0B;</p>
<pre><code>__author__ = &apos;CQC&apos;
# -*- coding:utf-8 -*-

import urllib
import urllib2
import re

class Spider:

    def __init__(self):
        self.siteURL = &apos;http://mm.taobao.com/json/request_top_list.htm&apos;

    def getPage(self,pageIndex):
        url = self.siteURL + &quot;?page=&quot; + str(pageIndex)
        print url
        request = urllib2.Request(url)
        response = urllib2.urlopen(request)
        return response.read().decode(&apos;gbk&apos;)

    def getContents(self,pageIndex):
        page = self.getPage(pageIndex)
        pattern = re.compile(&apos;&lt;div class=&quot;list-item&quot;.*?pic-word.*?&lt;a href=&quot;(.*?)&quot;.*?&lt;img src=&quot;(.*?)&quot;.*?&lt;a class=&quot;lady-name.*?&gt;(.*?)&lt;/a&gt;.*?&lt;strong&gt;(.*?)&lt;/strong&gt;.*?&lt;span&gt;(.*?)&lt;/span&gt;&apos;,re.S)
        items = re.findall(pattern,page)
        for item in items:
            print item[0],item[1],item[2],item[3],item[4]

spider = Spider()
spider.getContents(1)
</code></pre><p>&#x8FD0;&#x884C;&#x7ED3;&#x679C;&#x5982;&#x4E0B;</p>
<p><img src="../image/chapter2/section4-1.jpg" alt=""></p>
<h2 id="2&#x6587;&#x4EF6;&#x5199;&#x5165;&#x7B80;&#x4ECB;">2.&#x6587;&#x4EF6;&#x5199;&#x5165;&#x7B80;&#x4ECB;</h2>
<p>&#x5728;&#x8FD9;&#x91CC;&#xFF0C;&#x6211;&#x4EEC;&#x6709;&#x5199;&#x5165;&#x56FE;&#x7247;&#x548C;&#x5199;&#x5165;&#x6587;&#x672C;&#x4E24;&#x79CD;&#x65B9;&#x5F0F;</p>
<h3 id="1&#xFF09;&#x5199;&#x5165;&#x56FE;&#x7247;">1&#xFF09;&#x5199;&#x5165;&#x56FE;&#x7247;</h3>
<pre><code>#&#x4F20;&#x5165;&#x56FE;&#x7247;&#x5730;&#x5740;&#xFF0C;&#x6587;&#x4EF6;&#x540D;&#xFF0C;&#x4FDD;&#x5B58;&#x5355;&#x5F20;&#x56FE;&#x7247;
def saveImg(self,imageURL,fileName):
     u = urllib.urlopen(imageURL)
     data = u.read()
     f = open(fileName, &apos;wb&apos;)
     f.write(data)
     f.close()
</code></pre><h3 id="2&#xFF09;&#x5199;&#x5165;&#x6587;&#x672C;">2&#xFF09;&#x5199;&#x5165;&#x6587;&#x672C;</h3>
<pre><code>def saveBrief(self,content,name):
    fileName = name + &quot;/&quot; + name + &quot;.txt&quot;
    f = open(fileName,&quot;w+&quot;)
    print u&quot;&#x6B63;&#x5728;&#x5077;&#x5077;&#x4FDD;&#x5B58;&#x5979;&#x7684;&#x4E2A;&#x4EBA;&#x4FE1;&#x606F;&#x4E3A;&quot;,fileName
    f.write(content.encode(&apos;utf-8&apos;))
</code></pre><h3 id="3&#xFF09;&#x521B;&#x5EFA;&#x65B0;&#x76EE;&#x5F55;">3&#xFF09;&#x521B;&#x5EFA;&#x65B0;&#x76EE;&#x5F55;</h3>
<pre><code>#&#x521B;&#x5EFA;&#x65B0;&#x76EE;&#x5F55;
def mkdir(self,path):
    path = path.strip()
    # &#x5224;&#x65AD;&#x8DEF;&#x5F84;&#x662F;&#x5426;&#x5B58;&#x5728;
    # &#x5B58;&#x5728;     True
    # &#x4E0D;&#x5B58;&#x5728;   False
    isExists=os.path.exists(path)
    # &#x5224;&#x65AD;&#x7ED3;&#x679C;
    if not isExists:
        # &#x5982;&#x679C;&#x4E0D;&#x5B58;&#x5728;&#x5219;&#x521B;&#x5EFA;&#x76EE;&#x5F55;
        # &#x521B;&#x5EFA;&#x76EE;&#x5F55;&#x64CD;&#x4F5C;&#x51FD;&#x6570;
        os.makedirs(path)
        return True
    else:
        # &#x5982;&#x679C;&#x76EE;&#x5F55;&#x5B58;&#x5728;&#x5219;&#x4E0D;&#x521B;&#x5EFA;&#xFF0C;&#x5E76;&#x63D0;&#x793A;&#x76EE;&#x5F55;&#x5DF2;&#x5B58;&#x5728;
        return False
</code></pre><h2 id="3&#x4EE3;&#x7801;&#x5B8C;&#x5584;">3.&#x4EE3;&#x7801;&#x5B8C;&#x5584;</h2>
<p>&#x4E3B;&#x8981;&#x7684;&#x77E5;&#x8BC6;&#x70B9;&#x5DF2;&#x7ECF;&#x5728;&#x524D;&#x9762;&#x90FD;&#x6D89;&#x53CA;&#x5230;&#x4E86;&#xFF0C;&#x5982;&#x679C;&#x5927;&#x5BB6;&#x524D;&#x9762;&#x7684;&#x7AE0;&#x8282;&#x90FD;&#x5DF2;&#x7ECF;&#x770B;&#x4E86;&#xFF0C;&#x5B8C;&#x6210;&#x8FD9;&#x4E2A;&#x722C;&#x866B;&#x4E0D;&#x5728;&#x8BDD;&#x4E0B;&#xFF0C;&#x5177;&#x4F53;&#x7684;&#x8BE6;&#x60C5;&#x5728;&#x6B64;&#x4E0D;&#x518D;&#x8D58;&#x8FF0;&#xFF0C;&#x76F4;&#x63A5;&#x5E16;&#x4EE3;&#x7801;&#x5566;&#x3002;</p>
<pre><code>spider.py
</code></pre><pre><code>__author__ = &apos;CQC&apos;
# -*- coding:utf-8 -*-

import urllib
import urllib2
import re
import tool
import os

#&#x6293;&#x53D6;MM
class Spider:

    #&#x9875;&#x9762;&#x521D;&#x59CB;&#x5316;
    def __init__(self):
        self.siteURL = &apos;http://mm.taobao.com/json/request_top_list.htm&apos;
        self.tool = tool.Tool()

    #&#x83B7;&#x53D6;&#x7D22;&#x5F15;&#x9875;&#x9762;&#x7684;&#x5185;&#x5BB9;
    def getPage(self,pageIndex):
        url = self.siteURL + &quot;?page=&quot; + str(pageIndex)
        request = urllib2.Request(url)
        response = urllib2.urlopen(request)
        return response.read().decode(&apos;gbk&apos;)

    #&#x83B7;&#x53D6;&#x7D22;&#x5F15;&#x754C;&#x9762;&#x6240;&#x6709;MM&#x7684;&#x4FE1;&#x606F;&#xFF0C;list&#x683C;&#x5F0F;
    def getContents(self,pageIndex):
        page = self.getPage(pageIndex)
        pattern = re.compile(&apos;&lt;div class=&quot;list-item&quot;.*?pic-word.*?&lt;a href=&quot;(.*?)&quot;.*?&lt;img src=&quot;(.*?)&quot;.*?&lt;a class=&quot;lady-name.*?&gt;(.*?)&lt;/a&gt;.*?&lt;strong&gt;(.*?)&lt;/strong&gt;.*?&lt;span&gt;(.*?)&lt;/span&gt;&apos;,re.S)
        items = re.findall(pattern,page)
        contents = []
        for item in items:
            contents.append([item[0],item[1],item[2],item[3],item[4]])
        return contents

    #&#x83B7;&#x53D6;MM&#x4E2A;&#x4EBA;&#x8BE6;&#x60C5;&#x9875;&#x9762;
    def getDetailPage(self,infoURL):
        response = urllib2.urlopen(infoURL)
        return response.read().decode(&apos;gbk&apos;)

    #&#x83B7;&#x53D6;&#x4E2A;&#x4EBA;&#x6587;&#x5B57;&#x7B80;&#x4ECB;
    def getBrief(self,page):
        pattern = re.compile(&apos;&lt;div class=&quot;mm-aixiu-content&quot;.*?&gt;(.*?)&lt;!--&apos;,re.S)
        result = re.search(pattern,page)
        return self.tool.replace(result.group(1))

    #&#x83B7;&#x53D6;&#x9875;&#x9762;&#x6240;&#x6709;&#x56FE;&#x7247;
    def getAllImg(self,page):
        pattern = re.compile(&apos;&lt;div class=&quot;mm-aixiu-content&quot;.*?&gt;(.*?)&lt;!--&apos;,re.S)
        #&#x4E2A;&#x4EBA;&#x4FE1;&#x606F;&#x9875;&#x9762;&#x6240;&#x6709;&#x4EE3;&#x7801;
        content = re.search(pattern,page)
        #&#x4ECE;&#x4EE3;&#x7801;&#x4E2D;&#x63D0;&#x53D6;&#x56FE;&#x7247;
        patternImg = re.compile(&apos;&lt;img.*?src=&quot;(.*?)&quot;&apos;,re.S)
        images = re.findall(patternImg,content.group(1))
        return images


    #&#x4FDD;&#x5B58;&#x591A;&#x5F20;&#x5199;&#x771F;&#x56FE;&#x7247;
    def saveImgs(self,images,name):
        number = 1
        print u&quot;&#x53D1;&#x73B0;&quot;,name,u&quot;&#x5171;&#x6709;&quot;,len(images),u&quot;&#x5F20;&#x7167;&#x7247;&quot;
        for imageURL in images:
            splitPath = imageURL.split(&apos;.&apos;)
            fTail = splitPath.pop()
            if len(fTail) &gt; 3:
                fTail = &quot;jpg&quot;
            fileName = name + &quot;/&quot; + str(number) + &quot;.&quot; + fTail
            self.saveImg(imageURL,fileName)
            number += 1

    # &#x4FDD;&#x5B58;&#x5934;&#x50CF;
    def saveIcon(self,iconURL,name):
        splitPath = iconURL.split(&apos;.&apos;)
        fTail = splitPath.pop()
        fileName = name + &quot;/icon.&quot; + fTail
        self.saveImg(iconURL,fileName)

    #&#x4FDD;&#x5B58;&#x4E2A;&#x4EBA;&#x7B80;&#x4ECB;
    def saveBrief(self,content,name):
        fileName = name + &quot;/&quot; + name + &quot;.txt&quot;
        f = open(fileName,&quot;w+&quot;)
        print u&quot;&#x6B63;&#x5728;&#x5077;&#x5077;&#x4FDD;&#x5B58;&#x5979;&#x7684;&#x4E2A;&#x4EBA;&#x4FE1;&#x606F;&#x4E3A;&quot;,fileName
        f.write(content.encode(&apos;utf-8&apos;))


    #&#x4F20;&#x5165;&#x56FE;&#x7247;&#x5730;&#x5740;&#xFF0C;&#x6587;&#x4EF6;&#x540D;&#xFF0C;&#x4FDD;&#x5B58;&#x5355;&#x5F20;&#x56FE;&#x7247;
    def saveImg(self,imageURL,fileName):
         u = urllib.urlopen(imageURL)
         data = u.read()
         f = open(fileName, &apos;wb&apos;)
         f.write(data)
         print u&quot;&#x6B63;&#x5728;&#x6084;&#x6084;&#x4FDD;&#x5B58;&#x5979;&#x7684;&#x4E00;&#x5F20;&#x56FE;&#x7247;&#x4E3A;&quot;,fileName
         f.close()

    #&#x521B;&#x5EFA;&#x65B0;&#x76EE;&#x5F55;
    def mkdir(self,path):
        path = path.strip()
        # &#x5224;&#x65AD;&#x8DEF;&#x5F84;&#x662F;&#x5426;&#x5B58;&#x5728;
        # &#x5B58;&#x5728;     True
        # &#x4E0D;&#x5B58;&#x5728;   False
        isExists=os.path.exists(path)
        # &#x5224;&#x65AD;&#x7ED3;&#x679C;
        if not isExists:
            # &#x5982;&#x679C;&#x4E0D;&#x5B58;&#x5728;&#x5219;&#x521B;&#x5EFA;&#x76EE;&#x5F55;
            print u&quot;&#x5077;&#x5077;&#x65B0;&#x5EFA;&#x4E86;&#x540D;&#x5B57;&#x53EB;&#x505A;&quot;,path,u&apos;&#x7684;&#x6587;&#x4EF6;&#x5939;&apos;
            # &#x521B;&#x5EFA;&#x76EE;&#x5F55;&#x64CD;&#x4F5C;&#x51FD;&#x6570;
            os.makedirs(path)
            return True
        else:
            # &#x5982;&#x679C;&#x76EE;&#x5F55;&#x5B58;&#x5728;&#x5219;&#x4E0D;&#x521B;&#x5EFA;&#xFF0C;&#x5E76;&#x63D0;&#x793A;&#x76EE;&#x5F55;&#x5DF2;&#x5B58;&#x5728;
            print u&quot;&#x540D;&#x4E3A;&quot;,path,&apos;&#x7684;&#x6587;&#x4EF6;&#x5939;&#x5DF2;&#x7ECF;&#x521B;&#x5EFA;&#x6210;&#x529F;&apos;
            return False

    #&#x5C06;&#x4E00;&#x9875;&#x6DD8;&#x5B9D;MM&#x7684;&#x4FE1;&#x606F;&#x4FDD;&#x5B58;&#x8D77;&#x6765;
    def savePageInfo(self,pageIndex):
        #&#x83B7;&#x53D6;&#x7B2C;&#x4E00;&#x9875;&#x6DD8;&#x5B9D;MM&#x5217;&#x8868;
        contents = self.getContents(pageIndex)
        for item in contents:
            #item[0]&#x4E2A;&#x4EBA;&#x8BE6;&#x60C5;URL,item[1]&#x5934;&#x50CF;URL,item[2]&#x59D3;&#x540D;,item[3]&#x5E74;&#x9F84;,item[4]&#x5C45;&#x4F4F;&#x5730;
            print u&quot;&#x53D1;&#x73B0;&#x4E00;&#x4F4D;&#x6A21;&#x7279;,&#x540D;&#x5B57;&#x53EB;&quot;,item[2],u&quot;&#x82B3;&#x9F84;&quot;,item[3],u&quot;,&#x5979;&#x5728;&quot;,item[4]
            print u&quot;&#x6B63;&#x5728;&#x5077;&#x5077;&#x5730;&#x4FDD;&#x5B58;&quot;,item[2],&quot;&#x7684;&#x4FE1;&#x606F;&quot;
            print u&quot;&#x53C8;&#x610F;&#x5916;&#x5730;&#x53D1;&#x73B0;&#x5979;&#x7684;&#x4E2A;&#x4EBA;&#x5730;&#x5740;&#x662F;&quot;,item[0]
            #&#x4E2A;&#x4EBA;&#x8BE6;&#x60C5;&#x9875;&#x9762;&#x7684;URL
            detailURL = item[0]
            #&#x5F97;&#x5230;&#x4E2A;&#x4EBA;&#x8BE6;&#x60C5;&#x9875;&#x9762;&#x4EE3;&#x7801;
            detailPage = self.getDetailPage(detailURL)
            #&#x83B7;&#x53D6;&#x4E2A;&#x4EBA;&#x7B80;&#x4ECB;
            brief = self.getBrief(detailPage)
            #&#x83B7;&#x53D6;&#x6240;&#x6709;&#x56FE;&#x7247;&#x5217;&#x8868;
            images = self.getAllImg(detailPage)
            self.mkdir(item[2])
            #&#x4FDD;&#x5B58;&#x4E2A;&#x4EBA;&#x7B80;&#x4ECB;
            self.saveBrief(brief,item[2])
            #&#x4FDD;&#x5B58;&#x5934;&#x50CF;
            self.saveIcon(item[1],item[2])
            #&#x4FDD;&#x5B58;&#x56FE;&#x7247;
            self.saveImgs(images,item[2])

    #&#x4F20;&#x5165;&#x8D77;&#x6B62;&#x9875;&#x7801;&#xFF0C;&#x83B7;&#x53D6;MM&#x56FE;&#x7247;
    def savePagesInfo(self,start,end):
        for i in range(start,end+1):
            print u&quot;&#x6B63;&#x5728;&#x5077;&#x5077;&#x5BFB;&#x627E;&#x7B2C;&quot;,i,u&quot;&#x4E2A;&#x5730;&#x65B9;&#xFF0C;&#x770B;&#x770B;MM&#x4EEC;&#x5728;&#x4E0D;&#x5728;&quot;
            self.savePageInfo(i)


#&#x4F20;&#x5165;&#x8D77;&#x6B62;&#x9875;&#x7801;&#x5373;&#x53EF;&#xFF0C;&#x5728;&#x6B64;&#x4F20;&#x5165;&#x4E86;2,10,&#x8868;&#x793A;&#x6293;&#x53D6;&#x7B2C;2&#x5230;10&#x9875;&#x7684;MM
spider = Spider()
spider.savePagesInfo(2,10)
</code></pre><pre><code>tool.py
</code></pre><pre><code>__author__ = &apos;CQC&apos;
#-*- coding:utf-8 -*-
import re

#&#x5904;&#x7406;&#x9875;&#x9762;&#x6807;&#x7B7E;&#x7C7B;
class Tool:
    #&#x53BB;&#x9664;img&#x6807;&#x7B7E;,1-7&#x4F4D;&#x7A7A;&#x683C;,&amp;nbsp;
    removeImg = re.compile(&apos;&lt;img.*?&gt;| {1,7}|&amp;nbsp;&apos;)
    #&#x5220;&#x9664;&#x8D85;&#x94FE;&#x63A5;&#x6807;&#x7B7E;
    removeAddr = re.compile(&apos;&lt;a.*?&gt;|&lt;/a&gt;&apos;)
    #&#x628A;&#x6362;&#x884C;&#x7684;&#x6807;&#x7B7E;&#x6362;&#x4E3A;\n
    replaceLine = re.compile(&apos;&lt;tr&gt;|&lt;div&gt;|&lt;/div&gt;|&lt;/p&gt;&apos;)
    #&#x5C06;&#x8868;&#x683C;&#x5236;&#x8868;&lt;td&gt;&#x66FF;&#x6362;&#x4E3A;\t
    replaceTD= re.compile(&apos;&lt;td&gt;&apos;)
    #&#x5C06;&#x6362;&#x884C;&#x7B26;&#x6216;&#x53CC;&#x6362;&#x884C;&#x7B26;&#x66FF;&#x6362;&#x4E3A;\n
    replaceBR = re.compile(&apos;&lt;br&gt;&lt;br&gt;|&lt;br&gt;&apos;)
    #&#x5C06;&#x5176;&#x4F59;&#x6807;&#x7B7E;&#x5254;&#x9664;
    removeExtraTag = re.compile(&apos;&lt;.*?&gt;&apos;)
    #&#x5C06;&#x591A;&#x884C;&#x7A7A;&#x884C;&#x5220;&#x9664;
    removeNoneLine = re.compile(&apos;\n+&apos;)
    def replace(self,x):
        x = re.sub(self.removeImg,&quot;&quot;,x)
        x = re.sub(self.removeAddr,&quot;&quot;,x)
        x = re.sub(self.replaceLine,&quot;\n&quot;,x)
        x = re.sub(self.replaceTD,&quot;\t&quot;,x)
        x = re.sub(self.replaceBR,&quot;\n&quot;,x)
        x = re.sub(self.removeExtraTag,&quot;&quot;,x)
        x = re.sub(self.removeNoneLine,&quot;\n&quot;,x)
        #strip()&#x5C06;&#x524D;&#x540E;&#x591A;&#x4F59;&#x5185;&#x5BB9;&#x5220;&#x9664;
        return x.strip()
</code></pre><p>&#x4EE5;&#x4E0A;&#x4E24;&#x4E2A;&#x6587;&#x4EF6;&#x5C31;&#x662F;&#x6240;&#x6709;&#x7684;&#x4EE3;&#x7801;&#x5185;&#x5BB9;&#xFF0C;&#x8FD0;&#x884C;&#x4E00;&#x4E0B;&#x8BD5;&#x8BD5;&#x770B;&#xFF0C;&#x90A3;&#x53EB;&#x4E00;&#x4E2A;&#x9178;&#x723D;&#x554A;</p>
<p><img src="../image/chapter2/section4-2.jpg" alt=""></p>
<p>&#x770B;&#x770B;&#x6587;&#x4EF6;&#x5939;&#x91CC;&#x9762;&#x6709;&#x4EC0;&#x4E48;&#x53D8;&#x5316;</p>
<p><img src="../image/chapter2/section4-3.jpg" alt=""></p>
<p><img src="../image/chapter2/section4-4.jpg" alt=""></p>
<p>&#x4E0D;&#x77E5;&#x4E0D;&#x89C9;&#xFF0C;&#x6D77;&#x91CF;&#x7684;MM&#x56FE;&#x7247;&#x5DF2;&#x7ECF;&#x8FDB;&#x5165;&#x4E86;&#x4F60;&#x7684;&#x7535;&#x8111;&#xFF0C;&#x8FD8;&#x4E0D;&#x5FEB;&#x5FEB;&#x53BB;&#x8BD5;&#x8BD5;&#x770B;&#xFF01;&#xFF01;</p>
<p>&#x4EE3;&#x7801;&#x5747;&#x4E3A;&#x672C;&#x4EBA;&#x6240;&#x6572;&#xFF0C;&#x5199;&#x7684;&#x4E0D;&#x597D;&#xFF0C;&#x5927;&#x795E;&#x52FF;&#x55B7;&#xFF0C;&#x5199;&#x6765;&#x65B9;&#x4FBF;&#x81EA;&#x5DF1;&#xFF0C;&#x540C;&#x65F6;&#x5206;&#x4EAB;&#x7ED9;&#x5927;&#x5BB6;&#x53C2;&#x8003;&#xFF01;&#x5E0C;&#x671B;&#x5927;&#x5BB6;&#x652F;&#x6301;&#xFF01;</p>

                    
                    </section>
                
                
                </div>
            </div>
        </div>

        
        <a href="../chapter2/section3.html" class="navigation navigation-prev " aria-label="Previous page: 3. Python爬虫实战三之实现山东大学无线网络掉线自动重连"><i class="fa fa-angle-left"></i></a>
        
        
        <a href="../chapter2/section5.html" class="navigation navigation-next " aria-label="Next page: 5. Python爬虫实战五之模拟登录淘宝并获取所有订单"><i class="fa fa-angle-right"></i></a>
        
    </div>
</div>

        
<script src="../gitbook/app.js"></script>

    
    <script src="../gitbook/plugins/gitbook-plugin-search/lunr.min.js"></script>
    

    
    <script src="../gitbook/plugins/gitbook-plugin-search/search.js"></script>
    

    
    <script src="../gitbook/plugins/gitbook-plugin-sharing/buttons.js"></script>
    

    
    <script src="../gitbook/plugins/gitbook-plugin-fontsettings/buttons.js"></script>
    

    
    <script src="../gitbook/plugins/gitbook-plugin-livereload/plugin.js"></script>
    

<script>
require(["gitbook"], function(gitbook) {
    var config = {"highlight":{},"search":{"maxIndexSize":1000000},"sharing":{"facebook":true,"twitter":true,"google":false,"weibo":false,"instapaper":false,"vk":false,"all":["facebook","google","twitter","weibo","instapaper"]},"fontsettings":{"theme":"white","family":"sans","size":2},"livereload":{}};
    gitbook.start(config);
});
</script>

        
    </body>
    
</html>
