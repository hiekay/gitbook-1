<!DOCTYPE HTML>
<html lang="en" >
    
    <head>
        
        <meta charset="UTF-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge" />
        <title>1. Python爬虫实战一之爬取糗事百科段子 | Python爬虫学习系列教程</title>
        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <meta name="description" content="">
        <meta name="generator" content="GitBook 2.6.7">
        
        
        <meta name="HandheldFriendly" content="true"/>
        <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
        <meta name="apple-mobile-web-app-capable" content="yes">
        <meta name="apple-mobile-web-app-status-bar-style" content="black">
        <link rel="apple-touch-icon-precomposed" sizes="152x152" href="../gitbook/images/apple-touch-icon-precomposed-152.png">
        <link rel="shortcut icon" href="../gitbook/images/favicon.ico" type="image/x-icon">
        
    <link rel="stylesheet" href="../gitbook/style.css">
    
        
        <link rel="stylesheet" href="../gitbook/plugins/gitbook-plugin-highlight/website.css">
        
    
        
        <link rel="stylesheet" href="../gitbook/plugins/gitbook-plugin-search/search.css">
        
    
        
        <link rel="stylesheet" href="../gitbook/plugins/gitbook-plugin-fontsettings/website.css">
        
    
    

        
    
    
    <link rel="next" href="../chapter2/section2.html" />
    
    
    <link rel="prev" href="../chapter2/index.html" />
    

        
    </head>
    <body>
        
        
    <div class="book"
        data-level="2.1"
        data-chapter-title="1. Python爬虫实战一之爬取糗事百科段子"
        data-filepath="chapter2/section1.md"
        data-basepath=".."
        data-revision="Wed Jun 21 2017 19:16:14 GMT+0800 (中国标准时间)"
        data-innerlanguage="">
    

<div class="book-summary">
    <nav role="navigation">
        <ul class="summary">
            
            
            
            

            

            
    
        <li class="chapter " data-level="0" data-path="index.html">
            
                
                    <a href="../index.html">
                
                        <i class="fa fa-check"></i>
                        
                        Python爬虫学习系列教程
                    </a>
            
            
        </li>
    
        <li class="chapter " data-level="1" data-path="chapter1/index.html">
            
                
                    <a href="../chapter1/index.html">
                
                        <i class="fa fa-check"></i>
                        
                            <b>1.</b>
                        
                        一、爬虫入门
                    </a>
            
            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.1" data-path="chapter1/section1.html">
            
                
                    <a href="../chapter1/section1.html">
                
                        <i class="fa fa-check"></i>
                        
                            <b>1.1.</b>
                        
                        1. Python爬虫入门一之综述
                    </a>
            
            
        </li>
    
        <li class="chapter " data-level="1.2" data-path="chapter1/section2.html">
            
                
                    <a href="../chapter1/section2.html">
                
                        <i class="fa fa-check"></i>
                        
                            <b>1.2.</b>
                        
                        2. Python爬虫入门二之爬虫基础了解
                    </a>
            
            
        </li>
    
        <li class="chapter " data-level="1.3" data-path="chapter1/section3.html">
            
                
                    <a href="../chapter1/section3.html">
                
                        <i class="fa fa-check"></i>
                        
                            <b>1.3.</b>
                        
                        3. Python爬虫入门三之Urllib库的基本使用
                    </a>
            
            
        </li>
    
        <li class="chapter " data-level="1.4" data-path="chapter1/section4.html">
            
                
                    <a href="../chapter1/section4.html">
                
                        <i class="fa fa-check"></i>
                        
                            <b>1.4.</b>
                        
                        4. Python爬虫入门四之Urllib库的高级用法
                    </a>
            
            
        </li>
    
        <li class="chapter " data-level="1.5" data-path="chapter1/section5.html">
            
                
                    <a href="../chapter1/section5.html">
                
                        <i class="fa fa-check"></i>
                        
                            <b>1.5.</b>
                        
                        5. Python爬虫入门五之URLError异常处理
                    </a>
            
            
        </li>
    
        <li class="chapter " data-level="1.6" data-path="chapter1/section6.html">
            
                
                    <a href="../chapter1/section6.html">
                
                        <i class="fa fa-check"></i>
                        
                            <b>1.6.</b>
                        
                        6. Python爬虫入门六之Cookie的使用
                    </a>
            
            
        </li>
    
        <li class="chapter " data-level="1.7" data-path="chapter1/section7.html">
            
                
                    <a href="../chapter1/section7.html">
                
                        <i class="fa fa-check"></i>
                        
                            <b>1.7.</b>
                        
                        7. Python爬虫入门七之正则表达式
                    </a>
            
            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="2" data-path="chapter2/index.html">
            
                
                    <a href="../chapter2/index.html">
                
                        <i class="fa fa-check"></i>
                        
                            <b>2.</b>
                        
                        二、爬虫实战
                    </a>
            
            
            <ul class="articles">
                
    
        <li class="chapter active" data-level="2.1" data-path="chapter2/section1.html">
            
                
                    <a href="../chapter2/section1.html">
                
                        <i class="fa fa-check"></i>
                        
                            <b>2.1.</b>
                        
                        1. Python爬虫实战一之爬取糗事百科段子
                    </a>
            
            
        </li>
    
        <li class="chapter " data-level="2.2" data-path="chapter2/section2.html">
            
                
                    <a href="../chapter2/section2.html">
                
                        <i class="fa fa-check"></i>
                        
                            <b>2.2.</b>
                        
                        2. Python爬虫实战二之爬取百度贴吧帖子
                    </a>
            
            
        </li>
    
        <li class="chapter " data-level="2.3" data-path="chapter2/section3.html">
            
                
                    <a href="../chapter2/section3.html">
                
                        <i class="fa fa-check"></i>
                        
                            <b>2.3.</b>
                        
                        3. Python爬虫实战三之实现山东大学无线网络掉线自动重连
                    </a>
            
            
        </li>
    
        <li class="chapter " data-level="2.4" data-path="chapter2/section4.html">
            
                
                    <a href="../chapter2/section4.html">
                
                        <i class="fa fa-check"></i>
                        
                            <b>2.4.</b>
                        
                        4. Python爬虫实战四之抓取淘宝MM照片
                    </a>
            
            
        </li>
    
        <li class="chapter " data-level="2.5" data-path="chapter2/section5.html">
            
                
                    <a href="../chapter2/section5.html">
                
                        <i class="fa fa-check"></i>
                        
                            <b>2.5.</b>
                        
                        5. Python爬虫实战五之模拟登录淘宝并获取所有订单
                    </a>
            
            
        </li>
    
        <li class="chapter " data-level="2.6" data-path="chapter2/section6.html">
            
                
                    <a href="../chapter2/section6.html">
                
                        <i class="fa fa-check"></i>
                        
                            <b>2.6.</b>
                        
                        6. Python爬虫实战六之抓取爱问知识人问题并保存至数据库
                    </a>
            
            
        </li>
    
        <li class="chapter " data-level="2.7" data-path="chapter2/section7.html">
            
                
                    <a href="../chapter2/section7.html">
                
                        <i class="fa fa-check"></i>
                        
                            <b>2.7.</b>
                        
                        7. Python爬虫实战七之计算大学本学期绩点
                    </a>
            
            
        </li>
    
        <li class="chapter " data-level="2.8" data-path="chapter2/section8.html">
            
                
                    <a href="../chapter2/section8.html">
                
                        <i class="fa fa-check"></i>
                        
                            <b>2.8.</b>
                        
                        8. Python爬虫实战八之利用Selenium抓取淘宝匿名旺旺
                    </a>
            
            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="3" data-path="chapter3/index.html">
            
                
                    <a href="../chapter3/index.html">
                
                        <i class="fa fa-check"></i>
                        
                            <b>3.</b>
                        
                        三、爬虫利器
                    </a>
            
            
            <ul class="articles">
                
    
        <li class="chapter " data-level="3.1" data-path="chapter3/section1.html">
            
                
                    <a href="../chapter3/section1.html">
                
                        <i class="fa fa-check"></i>
                        
                            <b>3.1.</b>
                        
                        1. Python爬虫利器一之Requests库的用法
                    </a>
            
            
        </li>
    
        <li class="chapter " data-level="3.2" data-path="chapter3/section2.html">
            
                
                    <a href="../chapter3/section2.html">
                
                        <i class="fa fa-check"></i>
                        
                            <b>3.2.</b>
                        
                        2. Python爬虫利器二之Beautiful Soup的用法
                    </a>
            
            
        </li>
    
        <li class="chapter " data-level="3.3" data-path="chapter3/section3.html">
            
                
                    <a href="../chapter3/section3.html">
                
                        <i class="fa fa-check"></i>
                        
                            <b>3.3.</b>
                        
                        3. Python爬虫利器三之Xpath语法与lxml库的用法
                    </a>
            
            
        </li>
    
        <li class="chapter " data-level="3.4" data-path="chapter3/section4.html">
            
                
                    <a href="../chapter3/section4.html">
                
                        <i class="fa fa-check"></i>
                        
                            <b>3.4.</b>
                        
                        4. Python爬虫利器四之PhantomJS的用法
                    </a>
            
            
        </li>
    
        <li class="chapter " data-level="3.5" data-path="chapter3/section5.html">
            
                
                    <a href="../chapter3/section5.html">
                
                        <i class="fa fa-check"></i>
                        
                            <b>3.5.</b>
                        
                        5. Python爬虫利器五之Selenium的用法
                    </a>
            
            
        </li>
    
        <li class="chapter " data-level="3.6" data-path="chapter3/section6.html">
            
                
                    <a href="../chapter3/section6.html">
                
                        <i class="fa fa-check"></i>
                        
                            <b>3.6.</b>
                        
                        6. Python爬虫利器六之PyQuery的用法
                    </a>
            
            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="4" data-path="chapter4/index.html">
            
                
                    <a href="../chapter4/index.html">
                
                        <i class="fa fa-check"></i>
                        
                            <b>4.</b>
                        
                        四、爬虫进阶
                    </a>
            
            
            <ul class="articles">
                
    
        <li class="chapter " data-level="4.1" data-path="chapter4/section1.html">
            
                
                    <a href="../chapter4/section1.html">
                
                        <i class="fa fa-check"></i>
                        
                            <b>4.1.</b>
                        
                        1. Python爬虫进阶一之爬虫框架概述
                    </a>
            
            
        </li>
    
        <li class="chapter " data-level="4.2" data-path="chapter4/section2.html">
            
                
                    <a href="../chapter4/section2.html">
                
                        <i class="fa fa-check"></i>
                        
                            <b>4.2.</b>
                        
                        2. Python爬虫进阶二之PySpider框架安装配置
                    </a>
            
            
        </li>
    
        <li class="chapter " data-level="4.3" data-path="chapter4/section3.html">
            
                
                    <a href="../chapter4/section3.html">
                
                        <i class="fa fa-check"></i>
                        
                            <b>4.3.</b>
                        
                        3. Python爬虫进阶三之爬虫框架Scrapy安装配置
                    </a>
            
            
        </li>
    
        <li class="chapter " data-level="4.4" data-path="chapter4/section4.html">
            
                
                    <a href="../chapter4/section4.html">
                
                        <i class="fa fa-check"></i>
                        
                            <b>4.4.</b>
                        
                        4. Python爬虫进阶四之PySpider的用法
                    </a>
            
            
        </li>
    
        <li class="chapter " data-level="4.5" data-path="chapter4/section5.html">
            
                
                    <a href="../chapter4/section5.html">
                
                        <i class="fa fa-check"></i>
                        
                            <b>4.5.</b>
                        
                        5. Python爬虫进阶五之多线程的用法
                    </a>
            
            
        </li>
    
        <li class="chapter " data-level="4.6" data-path="chapter4/section6.html">
            
                
                    <a href="../chapter4/section6.html">
                
                        <i class="fa fa-check"></i>
                        
                            <b>4.6.</b>
                        
                        6. Python爬虫进阶六之多进程的用法
                    </a>
            
            
        </li>
    
        <li class="chapter " data-level="4.7" data-path="chapter4/section7.html">
            
                
                    <a href="../chapter4/section7.html">
                
                        <i class="fa fa-check"></i>
                        
                            <b>4.7.</b>
                        
                        7. Python爬虫进阶七之设置ADSL拨号服务器代理
                    </a>
            
            
        </li>
    

            </ul>
            
        </li>
    


            
            <li class="divider"></li>
            <li>
                <a href="https://www.gitbook.com" target="blank" class="gitbook-link">
                    Published with GitBook
                </a>
            </li>
            
        </ul>
    </nav>
</div>

    <div class="book-body">
        <div class="body-inner">
            <div class="book-header" role="navigation">
    <!-- Actions Left -->
    

    <!-- Title -->
    <h1>
        <i class="fa fa-circle-o-notch fa-spin"></i>
        <a href="../" >Python爬虫学习系列教程</a>
    </h1>
</div>

            <div class="page-wrapper" tabindex="-1" role="main">
                <div class="page-inner">
                
                
                    <section class="normal" id="section-">
                    
                        <h1 id="python&#x722C;&#x866B;&#x5B9E;&#x6218;&#x4E00;&#x4E4B;&#x722C;&#x53D6;&#x7CD7;&#x4E8B;&#x767E;&#x79D1;&#x6BB5;&#x5B50;">Python&#x722C;&#x866B;&#x5B9E;&#x6218;&#x4E00;&#x4E4B;&#x722C;&#x53D6;&#x7CD7;&#x4E8B;&#x767E;&#x79D1;&#x6BB5;&#x5B50;</h1>
<p>&#x5927;&#x5BB6;&#x597D;&#xFF0C;&#x524D;&#x9762;&#x5165;&#x95E8;&#x5DF2;&#x7ECF;&#x8BF4;&#x4E86;&#x90A3;&#x4E48;&#x591A;&#x57FA;&#x7840;&#x77E5;&#x8BC6;&#x4E86;&#xFF0C;&#x4E0B;&#x9762;&#x6211;&#x4EEC;&#x505A;&#x51E0;&#x4E2A;&#x5B9E;&#x6218;&#x9879;&#x76EE;&#x6765;&#x6311;&#x6218;&#x4E00;&#x4E0B;&#x5427;&#x3002;&#x90A3;&#x4E48;&#x8FD9;&#x6B21;&#x4E3A;&#x5927;&#x5BB6;&#x5E26;&#x6765;&#xFF0C;Python&#x722C;&#x53D6;&#x7CD7;&#x4E8B;&#x767E;&#x79D1;&#x7684;&#x5C0F;&#x6BB5;&#x5B50;&#x7684;&#x4F8B;&#x5B50;&#x3002;</p>
<p>&#x9996;&#x5148;&#xFF0C;&#x7CD7;&#x4E8B;&#x767E;&#x79D1;&#x5927;&#x5BB6;&#x90FD;&#x542C;&#x8BF4;&#x8FC7;&#x5427;&#xFF1F;&#x7CD7;&#x53CB;&#x4EEC;&#x53D1;&#x7684;&#x641E;&#x7B11;&#x7684;&#x6BB5;&#x5B50;&#x4E00;&#x6293;&#x4E00;&#x5927;&#x628A;&#xFF0C;&#x8FD9;&#x6B21;&#x6211;&#x4EEC;&#x5C1D;&#x8BD5;&#x4E00;&#x4E0B;&#x7528;&#x722C;&#x866B;&#x628A;&#x4ED6;&#x4EEC;&#x6293;&#x53D6;&#x4E0B;&#x6765;&#x3002;</p>
<h2 id="&#x53CB;&#x60C5;&#x63D0;&#x793A;">&#x53CB;&#x60C5;&#x63D0;&#x793A;</h2>
<blockquote>
<p>&#x7CD7;&#x4E8B;&#x767E;&#x79D1;&#x5728;&#x524D;&#x4E00;&#x6BB5;&#x65F6;&#x95F4;&#x8FDB;&#x884C;&#x4E86;&#x6539;&#x7248;&#xFF0C;&#x5BFC;&#x81F4;&#x4E4B;&#x524D;&#x7684;&#x4EE3;&#x7801;&#x6CA1;&#x6CD5;&#x7528;&#x4E86;&#xFF0C;&#x4F1A;&#x5BFC;&#x81F4;&#x65E0;&#x6CD5;&#x8F93;&#x51FA;&#x548C;CPU&#x5360;&#x7528;&#x8FC7;&#x9AD8;&#x7684;&#x60C5;&#x51B5;&#xFF0C;&#x662F;&#x56E0;&#x4E3A;&#x6B63;&#x5219;&#x8868;&#x8FBE;&#x5F0F;&#x6CA1;&#x6709;&#x5339;&#x914D;&#x5230;&#x7684;&#x7F18;&#x6545;&#x3002;
&#x73B0;&#x5728;&#xFF0C;&#x535A;&#x4E3B;&#x5DF2;&#x7ECF;&#x5BF9;&#x7A0B;&#x5E8F;&#x8FDB;&#x884C;&#x4E86;&#x91CD;&#x65B0;&#x4FEE;&#x6539;&#xFF0C;&#x4EE3;&#x7801;&#x4EB2;&#x6D4B;&#x53EF;&#x7528;&#xFF0C;&#x5305;&#x62EC;&#x622A;&#x56FE;&#x548C;&#x8BF4;&#x660E;&#xFF0C;&#x4E4B;&#x524D;&#x4E00;&#x76F4;&#x5728;&#x5FD9;&#x6240;&#x4EE5;&#x6CA1;&#x6709;&#x53CA;&#x65F6;&#x66F4;&#x65B0;&#xFF0C;&#x671B;&#x5927;&#x5BB6;&#x6D77;&#x6DB5;&#xFF01;
&#x66F4;&#x65B0;&#x65F6;&#x95F4;&#xFF1A;2015/8/2</p>
<p>&#x7CD7;&#x4E8B;&#x767E;&#x79D1;&#x53C8;&#x53C8;&#x53C8;&#x53C8;&#x6539;&#x7248;&#x4E86;&#xFF0C;&#x535A;&#x4E3B;&#x5DF2;&#x7ECF;&#x6CA1;&#x5FC3;&#x518D;&#x53BB;&#x4E00;&#x6B21;&#x6B21;&#x5339;&#x914D;&#x5B83;&#x4E86;&#xFF0C;&#x5982;&#x679C;&#x5927;&#x5BB6;&#x9047;&#x5230;&#x957F;&#x65F6;&#x95F4;&#x8FD0;&#x884C;&#x4E0D;&#x51FA;&#x7ED3;&#x679C;&#x4E5F;&#x4E0D;&#x62A5;&#x9519;&#x7684;&#x60C5;&#x51B5;&#xFF0C;&#x8BF7;&#x5927;&#x5BB6;&#x53C2;&#x8003;&#x6700;&#x65B0;&#x7684;&#x8BC4;&#x8BBA;&#xFF0C;&#x70ED;&#x5FC3;&#x5C0F;&#x4F19;&#x4F34;&#x63D0;&#x4F9B;&#x7684;&#x6B63;&#x5219;&#x6765;&#x4FEE;&#x6539;&#x4E0B;&#x5427;&#xFF5E;
&#x66F4;&#x65B0;&#x65F6;&#x95F4;&#xFF1A;2016/3/27</p>
</blockquote>
<h2 id="&#x672C;&#x7BC7;&#x76EE;&#x6807;">&#x672C;&#x7BC7;&#x76EE;&#x6807;</h2>
<blockquote>
<ul>
<li>1.&#x6293;&#x53D6;&#x7CD7;&#x4E8B;&#x767E;&#x79D1;&#x70ED;&#x95E8;&#x6BB5;&#x5B50;</li>
<li>2.&#x8FC7;&#x6EE4;&#x5E26;&#x6709;&#x56FE;&#x7247;&#x7684;&#x6BB5;&#x5B50;</li>
<li>3.&#x5B9E;&#x73B0;&#x6BCF;&#x6309;&#x4E00;&#x6B21;&#x56DE;&#x8F66;&#x663E;&#x793A;&#x4E00;&#x4E2A;&#x6BB5;&#x5B50;&#x7684;&#x53D1;&#x5E03;&#x65F6;&#x95F4;&#xFF0C;&#x53D1;&#x5E03;&#x4EBA;&#xFF0C;&#x6BB5;&#x5B50;&#x5185;&#x5BB9;&#xFF0C;&#x70B9;&#x8D5E;&#x6570;&#x3002;</li>
</ul>
</blockquote>
<p>&#x7CD7;&#x4E8B;&#x767E;&#x79D1;&#x662F;&#x4E0D;&#x9700;&#x8981;&#x767B;&#x5F55;&#x7684;&#xFF0C;&#x6240;&#x4EE5;&#x4E5F;&#x6CA1;&#x5FC5;&#x8981;&#x7528;&#x5230;Cookie&#xFF0C;&#x53E6;&#x5916;&#x7CD7;&#x4E8B;&#x767E;&#x79D1;&#x6709;&#x7684;&#x6BB5;&#x5B50;&#x662F;&#x9644;&#x56FE;&#x7684;&#xFF0C;&#x6211;&#x4EEC;&#x628A;&#x56FE;&#x6293;&#x4E0B;&#x6765;&#x56FE;&#x7247;&#x4E0D;&#x4FBF;&#x4E8E;&#x663E;&#x793A;&#xFF0C;&#x90A3;&#x4E48;&#x6211;&#x4EEC;&#x5C31;&#x5C1D;&#x8BD5;&#x8FC7;&#x6EE4;&#x6389;&#x6709;&#x56FE;&#x7684;&#x6BB5;&#x5B50;&#x5427;&#x3002;</p>
<p>&#x597D;&#xFF0C;&#x73B0;&#x5728;&#x6211;&#x4EEC;&#x5C1D;&#x8BD5;&#x6293;&#x53D6;&#x4E00;&#x4E0B;&#x7CD7;&#x4E8B;&#x767E;&#x79D1;&#x7684;&#x70ED;&#x95E8;&#x6BB5;&#x5B50;&#x5427;&#xFF0C;&#x6BCF;&#x6309;&#x4E0B;&#x4E00;&#x6B21;&#x56DE;&#x8F66;&#x6211;&#x4EEC;&#x663E;&#x793A;&#x4E00;&#x4E2A;&#x6BB5;&#x5B50;&#x3002;</p>
<h3 id="1&#x786E;&#x5B9A;url&#x5E76;&#x6293;&#x53D6;&#x9875;&#x9762;&#x4EE3;&#x7801;">1.&#x786E;&#x5B9A;URL&#x5E76;&#x6293;&#x53D6;&#x9875;&#x9762;&#x4EE3;&#x7801;</h3>
<p>&#x9996;&#x5148;&#x6211;&#x4EEC;&#x786E;&#x5B9A;&#x597D;&#x9875;&#x9762;&#x7684;URL&#x662F; <a href="http://www.qiushibaike.com/hot/page/1&#xFF0C;&#x5176;&#x4E2D;&#x6700;&#x540E;&#x4E00;&#x4E2A;&#x6570;&#x5B57;1&#x4EE3;&#x8868;&#x9875;&#x6570;&#xFF0C;&#x6211;&#x4EEC;&#x53EF;&#x4EE5;&#x4F20;&#x5165;&#x4E0D;&#x540C;&#x7684;&#x503C;&#x6765;&#x83B7;&#x5F97;&#x67D0;&#x4E00;&#x9875;&#x7684;&#x6BB5;&#x5B50;&#x5185;&#x5BB9;&#x3002;" target="_blank">http://www.qiushibaike.com/hot/page/1&#xFF0C;&#x5176;&#x4E2D;&#x6700;&#x540E;&#x4E00;&#x4E2A;&#x6570;&#x5B57;1&#x4EE3;&#x8868;&#x9875;&#x6570;&#xFF0C;&#x6211;&#x4EEC;&#x53EF;&#x4EE5;&#x4F20;&#x5165;&#x4E0D;&#x540C;&#x7684;&#x503C;&#x6765;&#x83B7;&#x5F97;&#x67D0;&#x4E00;&#x9875;&#x7684;&#x6BB5;&#x5B50;&#x5185;&#x5BB9;&#x3002;</a></p>
<p>&#x6211;&#x4EEC;&#x521D;&#x6B65;&#x6784;&#x5EFA;&#x5982;&#x4E0B;&#x7684;&#x4EE3;&#x7801;&#x6765;&#x6253;&#x5370;&#x9875;&#x9762;&#x4EE3;&#x7801;&#x5185;&#x5BB9;&#x8BD5;&#x8BD5;&#x770B;&#xFF0C;&#x5148;&#x6784;&#x9020;&#x6700;&#x57FA;&#x672C;&#x7684;&#x9875;&#x9762;&#x6293;&#x53D6;&#x65B9;&#x5F0F;&#xFF0C;&#x770B;&#x770B;&#x4F1A;&#x4E0D;&#x4F1A;&#x6210;&#x529F;</p>
<pre><code># -*- coding:utf-8 -*-
import urllib
import urllib2


page = 1
url = &apos;http://www.qiushibaike.com/hot/page/&apos; + str(page)
try:
    request = urllib2.Request(url)
    response = urllib2.urlopen(request)
    print response.read()
except urllib2.URLError, e:
    if hasattr(e,&quot;code&quot;):
        print e.code
    if hasattr(e,&quot;reason&quot;):
        print e.reason
</code></pre><p>&#x8FD0;&#x884C;&#x7A0B;&#x5E8F;&#xFF0C;&#x54E6;&#x4E0D;&#xFF0C;&#x5B83;&#x7ADF;&#x7136;&#x62A5;&#x9519;&#x4E86;&#xFF0C;&#x771F;&#x662F;&#x65F6;&#x8FD0;&#x4E0D;&#x6D4E;&#xFF0C;&#x547D;&#x9014;&#x591A;&#x821B;&#x554A;</p>
<pre><code>line 373, in _read_status
 raise BadStatusLine(line)
httplib.BadStatusLine: &apos;&apos;
</code></pre><p>&#x597D;&#x5427;&#xFF0C;&#x5E94;&#x8BE5;&#x662F;headers&#x9A8C;&#x8BC1;&#x7684;&#x95EE;&#x9898;&#xFF0C;&#x6211;&#x4EEC;&#x52A0;&#x4E0A;&#x4E00;&#x4E2A;headers&#x9A8C;&#x8BC1;&#x8BD5;&#x8BD5;&#x770B;&#x5427;&#xFF0C;&#x5C06;&#x4EE3;&#x7801;&#x4FEE;&#x6539;&#x5982;&#x4E0B;</p>
<pre><code># -*- coding:utf-8 -*-
import urllib
import urllib2

page = 1
url = &apos;http://www.qiushibaike.com/hot/page/&apos; + str(page)
user_agent = &apos;Mozilla/4.0 (compatible; MSIE 5.5; Windows NT)&apos;
headers = { &apos;User-Agent&apos; : user_agent }
try:
    request = urllib2.Request(url,headers = headers)
    response = urllib2.urlopen(request)
    print response.read()
except urllib2.URLError, e:
    if hasattr(e,&quot;code&quot;):
        print e.code
    if hasattr(e,&quot;reason&quot;):
        print e.reason
</code></pre><p>&#x563F;&#x563F;&#xFF0C;&#x8FD9;&#x6B21;&#x8FD0;&#x884C;&#x7EC8;&#x4E8E;&#x6B63;&#x5E38;&#x4E86;&#xFF0C;&#x6253;&#x5370;&#x51FA;&#x4E86;&#x7B2C;&#x4E00;&#x9875;&#x7684;HTML&#x4EE3;&#x7801;&#xFF0C;&#x5927;&#x5BB6;&#x53EF;&#x4EE5;&#x8FD0;&#x884C;&#x4E0B;&#x4EE3;&#x7801;&#x8BD5;&#x8BD5;&#x770B;&#x3002;&#x5728;&#x8FD9;&#x91CC;&#x8FD0;&#x884C;&#x7ED3;&#x679C;&#x592A;&#x957F;&#x5C31;&#x4E0D;&#x8D34;&#x4E86;&#x3002;</p>
<h3 id="2&#x63D0;&#x53D6;&#x67D0;&#x4E00;&#x9875;&#x7684;&#x6240;&#x6709;&#x6BB5;&#x5B50;">2.&#x63D0;&#x53D6;&#x67D0;&#x4E00;&#x9875;&#x7684;&#x6240;&#x6709;&#x6BB5;&#x5B50;</h3>
<p>&#x597D;&#xFF0C;&#x83B7;&#x53D6;&#x4E86;HTML&#x4EE3;&#x7801;&#x4E4B;&#x540E;&#xFF0C;&#x6211;&#x4EEC;&#x5F00;&#x59CB;&#x5206;&#x6790;&#x600E;&#x6837;&#x83B7;&#x53D6;&#x67D0;&#x4E00;&#x9875;&#x7684;&#x6240;&#x6709;&#x6BB5;&#x5B50;&#x3002;</p>
<p>&#x9996;&#x5148;&#x6211;&#x4EEC;&#x5BA1;&#x67E5;&#x5143;&#x7D20;&#x770B;&#x4E00;&#x4E0B;&#xFF0C;&#x6309;&#x6D4F;&#x89C8;&#x5668;&#x7684;F12&#xFF0C;&#x622A;&#x56FE;&#x5982;&#x4E0B;</p>
<p><img src="../image/chapter2/section1-1.jpg" alt=""></p>
<p>&#x6211;&#x4EEC;&#x53EF;&#x4EE5;&#x770B;&#x5230;&#xFF0C;&#x6BCF;&#x4E00;&#x4E2A;&#x6BB5;&#x5B50;&#x90FD;&#x662F;<div class="&#x201D;article" block="" untagged="" mb15″="" id="&#x201D;&#x2026;&#x201D;">&#x2026;</div>&#x5305;&#x88F9;&#x7684;&#x5185;&#x5BB9;&#x3002;</p>
<p>&#x73B0;&#x5728;&#x6211;&#x4EEC;&#x60F3;&#x83B7;&#x53D6;&#x53D1;&#x5E03;&#x4EBA;&#xFF0C;&#x53D1;&#x5E03;&#x65E5;&#x671F;&#xFF0C;&#x6BB5;&#x5B50;&#x5185;&#x5BB9;&#xFF0C;&#x4EE5;&#x53CA;&#x70B9;&#x8D5E;&#x7684;&#x4E2A;&#x6570;&#x3002;&#x4E0D;&#x8FC7;&#x53E6;&#x5916;&#x6CE8;&#x610F;&#x7684;&#x662F;&#xFF0C;&#x6BB5;&#x5B50;&#x6709;&#x4E9B;&#x662F;&#x5E26;&#x56FE;&#x7247;&#x7684;&#xFF0C;&#x5982;&#x679C;&#x6211;&#x4EEC;&#x60F3;&#x5728;&#x63A7;&#x5236;&#x53F0;&#x663E;&#x793A;&#x56FE;&#x7247;&#x662F;&#x4E0D;&#x73B0;&#x5B9E;&#x7684;&#xFF0C;&#x6240;&#x4EE5;&#x6211;&#x4EEC;&#x76F4;&#x63A5;&#x628A;&#x5E26;&#x6709;&#x56FE;&#x7247;&#x7684;&#x6BB5;&#x5B50;&#x7ED9;&#x5B83;&#x5254;&#x9664;&#x6389;&#xFF0C;&#x53EA;&#x4FDD;&#x5B58;&#x4EC5;&#x542B;&#x6587;&#x672C;&#x7684;&#x6BB5;&#x5B50;&#x3002;</p>
<p>&#x6240;&#x4EE5;&#x6211;&#x4EEC;&#x52A0;&#x5165;&#x5982;&#x4E0B;&#x6B63;&#x5219;&#x8868;&#x8FBE;&#x5F0F;&#x6765;&#x5339;&#x914D;&#x4E00;&#x4E0B;&#xFF0C;&#x7528;&#x5230;&#x7684;&#x65B9;&#x6CD5;&#x662F; re.findall &#x662F;&#x627E;&#x5BFB;&#x6240;&#x6709;&#x5339;&#x914D;&#x7684;&#x5185;&#x5BB9;&#x3002;&#x65B9;&#x6CD5;&#x7684;&#x7528;&#x6CD5;&#x8BE6;&#x60C5;&#x53EF;&#x4EE5;&#x770B;&#x524D;&#x9762;&#x8BF4;&#x7684;&#x6B63;&#x5219;&#x8868;&#x8FBE;&#x5F0F;&#x7684;&#x4ECB;&#x7ECD;&#x3002;</p>
<p>&#x597D;&#xFF0C;&#x6211;&#x4EEC;&#x7684;&#x6B63;&#x5219;&#x8868;&#x8FBE;&#x5F0F;&#x5339;&#x914D;&#x8BED;&#x53E5;&#x4E66;&#x5199;&#x5982;&#x4E0B;&#xFF0C;&#x5728;&#x539F;&#x6765;&#x7684;&#x57FA;&#x7840;&#x4E0A;&#x8FFD;&#x52A0;&#x5982;&#x4E0B;&#x4EE3;&#x7801;</p>
<pre><code>content = response.read().decode(&apos;utf-8&apos;)
pattern = re.compile(&apos;&lt;div.*?author&quot;&gt;.*?&lt;a.*?&lt;img.*?&gt;(.*?)&lt;/a&gt;.*?&lt;div.*?&apos;+
                         &apos;content&quot;&gt;(.*?)&lt;!--(.*?)--&gt;.*?&lt;/div&gt;(.*?)&lt;div class=&quot;stats.*?class=&quot;number&quot;&gt;(.*?)&lt;/i&gt;&apos;,re.S)
items = re.findall(pattern,content)
for item in items:
    print item[0],item[1],item[2],item[3],item[4]
</code></pre><p>&#x73B0;&#x5728;&#x6B63;&#x5219;&#x8868;&#x8FBE;&#x5F0F;&#x5728;&#x8FD9;&#x91CC;&#x7A0D;&#x4F5C;&#x8BF4;&#x660E;</p>
<p>1&#xFF09;.<em>? &#x662F;&#x4E00;&#x4E2A;&#x56FA;&#x5B9A;&#x7684;&#x642D;&#x914D;&#xFF0C;.&#x548C;</em>&#x4EE3;&#x8868;&#x53EF;&#x4EE5;&#x5339;&#x914D;&#x4EFB;&#x610F;&#x65E0;&#x9650;&#x591A;&#x4E2A;&#x5B57;&#x7B26;&#xFF0C;&#x52A0;&#x4E0A;&#xFF1F;&#x8868;&#x793A;&#x4F7F;&#x7528;&#x975E;&#x8D2A;&#x5A6A;&#x6A21;&#x5F0F;&#x8FDB;&#x884C;&#x5339;&#x914D;&#xFF0C;&#x4E5F;&#x5C31;&#x662F;&#x6211;&#x4EEC;&#x4F1A;&#x5C3D;&#x53EF;&#x80FD;&#x77ED;&#x5730;&#x505A;&#x5339;&#x914D;&#xFF0C;&#x4EE5;&#x540E;&#x6211;&#x4EEC;&#x8FD8;&#x4F1A;&#x5927;&#x91CF;&#x7528;&#x5230; .*? &#x7684;&#x642D;&#x914D;&#x3002;</p>
<p>2&#xFF09;(.<em>?)&#x4EE3;&#x8868;&#x4E00;&#x4E2A;&#x5206;&#x7EC4;&#xFF0C;&#x5728;&#x8FD9;&#x4E2A;&#x6B63;&#x5219;&#x8868;&#x8FBE;&#x5F0F;&#x4E2D;&#x6211;&#x4EEC;&#x5339;&#x914D;&#x4E86;&#x4E94;&#x4E2A;&#x5206;&#x7EC4;&#xFF0C;&#x5728;&#x540E;&#x9762;&#x7684;&#x904D;&#x5386;item&#x4E2D;&#xFF0C;item[0]&#x5C31;&#x4EE3;&#x8868;&#x7B2C;&#x4E00;&#x4E2A;(.</em>?)&#x6240;&#x6307;&#x4EE3;&#x7684;&#x5185;&#x5BB9;&#xFF0C;item[1]&#x5C31;&#x4EE3;&#x8868;&#x7B2C;&#x4E8C;&#x4E2A;(.*?)&#x6240;&#x6307;&#x4EE3;&#x7684;&#x5185;&#x5BB9;&#xFF0C;&#x4EE5;&#x6B64;&#x7C7B;&#x63A8;&#x3002;</p>
<p>3&#xFF09;re.S &#x6807;&#x5FD7;&#x4EE3;&#x8868;&#x5728;&#x5339;&#x914D;&#x65F6;&#x4E3A;&#x70B9;&#x4EFB;&#x610F;&#x5339;&#x914D;&#x6A21;&#x5F0F;&#xFF0C;&#x70B9; . &#x4E5F;&#x53EF;&#x4EE5;&#x4EE3;&#x8868;&#x6362;&#x884C;&#x7B26;&#x3002;</p>
<p>&#x8FD9;&#x6837;&#x6211;&#x4EEC;&#x5C31;&#x83B7;&#x53D6;&#x4E86;&#x53D1;&#x5E03;&#x4EBA;&#xFF0C;&#x53D1;&#x5E03;&#x65F6;&#x95F4;&#xFF0C;&#x53D1;&#x5E03;&#x5185;&#x5BB9;&#xFF0C;&#x9644;&#x52A0;&#x56FE;&#x7247;&#x4EE5;&#x53CA;&#x70B9;&#x8D5E;&#x6570;&#x3002;</p>
<p>&#x5728;&#x8FD9;&#x91CC;&#x6CE8;&#x610F;&#x4E00;&#x4E0B;&#xFF0C;&#x6211;&#x4EEC;&#x8981;&#x83B7;&#x53D6;&#x7684;&#x5185;&#x5BB9;&#x5982;&#x679C;&#x662F;&#x5E26;&#x6709;&#x56FE;&#x7247;&#xFF0C;&#x76F4;&#x63A5;&#x8F93;&#x51FA;&#x51FA;&#x6765;&#x6BD4;&#x8F83;&#x7E41;&#x7410;&#xFF0C;&#x6240;&#x4EE5;&#x8FD9;&#x91CC;&#x6211;&#x4EEC;&#x53EA;&#x83B7;&#x53D6;&#x4E0D;&#x5E26;&#x56FE;&#x7247;&#x7684;&#x6BB5;&#x5B50;&#x5C31;&#x597D;&#x4E86;&#x3002;</p>
<p>&#x6240;&#x4EE5;&#xFF0C;&#x5728;&#x8FD9;&#x91CC;&#x6211;&#x4EEC;&#x5C31;&#x9700;&#x8981;&#x5BF9;&#x5E26;&#x56FE;&#x7247;&#x7684;&#x6BB5;&#x5B50;&#x8FDB;&#x884C;&#x8FC7;&#x6EE4;&#x3002;</p>
<p>&#x6211;&#x4EEC;&#x53EF;&#x4EE5;&#x53D1;&#x73B0;&#xFF0C;&#x5E26;&#x6709;&#x56FE;&#x7247;&#x7684;&#x6BB5;&#x5B50;&#x4F1A;&#x5E26;&#x6709;&#x7C7B;&#x4F3C;&#x4E0B;&#x9762;&#x7684;&#x4EE3;&#x7801;&#xFF0C;&#x800C;&#x4E0D;&#x5E26;&#x56FE;&#x7247;&#x7684;&#x5219;&#x6CA1;&#x6709;&#xFF0C;&#x6240;&#x4EE5;&#xFF0C;&#x6211;&#x4EEC;&#x7684;&#x6B63;&#x5219;&#x8868;&#x8FBE;&#x5F0F;&#x7684;item[3]&#x5C31;&#x662F;&#x83B7;&#x53D6;&#x4E86;&#x4E0B;&#x9762;&#x7684;&#x5185;&#x5BB9;&#xFF0C;&#x5982;&#x679C;&#x4E0D;&#x5E26;&#x56FE;&#x7247;&#xFF0C;item[3]&#x83B7;&#x53D6;&#x7684;&#x5185;&#x5BB9;&#x4FBF;&#x662F;&#x7A7A;&#x3002;</p>
<pre><code>&lt;div class=&quot;thumb&quot;&gt;

&lt;a href=&quot;/article/112061287?list=hot&amp;amp;s=4794990&quot; target=&quot;_blank&quot;&gt;
&lt;img src=&quot;http://pic.qiushibaike.com/system/pictures/11206/112061287/medium/app112061287.jpg&quot; alt=&quot;&#x4F46;&#x4ED6;&#x4EEC;&#x4F9D;&#x7136;&#x4E50;&#x89C2;&quot;&gt;
&lt;/a&gt;

&lt;/div&gt;
</code></pre><p>&#x6240;&#x4EE5;&#x6211;&#x4EEC;&#x53EA;&#x9700;&#x8981;&#x5224;&#x65AD;item[3]&#x4E2D;&#x662F;&#x5426;&#x542B;&#x6709;img&#x6807;&#x7B7E;&#x5C31;&#x53EF;&#x4EE5;&#x4E86;&#x3002;</p>
<p>&#x597D;&#xFF0C;&#x6211;&#x4EEC;&#x518D;&#x628A;&#x4E0A;&#x8FF0;&#x4EE3;&#x7801;&#x4E2D;&#x7684;for&#x5FAA;&#x73AF;&#x6539;&#x4E3A;&#x4E0B;&#x9762;&#x7684;&#x6837;&#x5B50;</p>
<pre><code>for item in items:
        haveImg = re.search(&quot;img&quot;,item[3])
        if not haveImg:
            print item[0],item[1],item[2],item[4]
</code></pre><p>&#x73B0;&#x5728;&#xFF0C;&#x6574;&#x4F53;&#x7684;&#x4EE3;&#x7801;&#x5982;&#x4E0B;</p>
<pre><code># -*- coding:utf-8 -*-
import urllib
import urllib2
import re

page = 1
url = &apos;http://www.qiushibaike.com/hot/page/&apos; + str(page)
user_agent = &apos;Mozilla/4.0 (compatible; MSIE 5.5; Windows NT)&apos;
headers = { &apos;User-Agent&apos; : user_agent }
try:
    request = urllib2.Request(url,headers = headers)
    response = urllib2.urlopen(request)
    content = response.read().decode(&apos;utf-8&apos;)
    pattern = re.compile(&apos;&lt;div.*?author&quot;&gt;.*?&lt;a.*?&lt;img.*?&gt;(.*?)&lt;/a&gt;.*?&lt;div.*?&apos;+
                         &apos;content&quot;&gt;(.*?)&lt;!--(.*?)--&gt;.*?&lt;/div&gt;(.*?)&lt;div class=&quot;stats.*?class=&quot;number&quot;&gt;(.*?)&lt;/i&gt;&apos;,re.S)
    items = re.findall(pattern,content)
    for item in items:
        haveImg = re.search(&quot;img&quot;,item[3])
        if not haveImg:
            print item[0],item[1],item[2],item[4]
except urllib2.URLError, e:
    if hasattr(e,&quot;code&quot;):
        print e.code
    if hasattr(e,&quot;reason&quot;):
        print e.reason
</code></pre><p>&#x8FD0;&#x884C;&#x4E00;&#x4E0B;&#x770B;&#x4E0B;&#x6548;&#x679C;</p>
<p><img src="../image/chapter2/section1-2.jpg" alt=""></p>
<p>&#x6069;&#xFF0C;&#x5E26;&#x6709;&#x56FE;&#x7247;&#x7684;&#x6BB5;&#x5B50;&#x5DF2;&#x7ECF;&#x88AB;&#x5254;&#x9664;&#x5566;&#x3002;&#x662F;&#x4E0D;&#x662F;&#x5F88;&#x5F00;&#x68EE;&#xFF1F;</p>
<h3 id="3&#x5B8C;&#x5584;&#x4EA4;&#x4E92;&#xFF0C;&#x8BBE;&#x8BA1;&#x9762;&#x5411;&#x5BF9;&#x8C61;&#x6A21;&#x5F0F;">3.&#x5B8C;&#x5584;&#x4EA4;&#x4E92;&#xFF0C;&#x8BBE;&#x8BA1;&#x9762;&#x5411;&#x5BF9;&#x8C61;&#x6A21;&#x5F0F;</h3>
<p>&#x597D;&#x5566;&#xFF0C;&#x73B0;&#x5728;&#x6700;&#x6838;&#x5FC3;&#x7684;&#x90E8;&#x5206;&#x6211;&#x4EEC;&#x5DF2;&#x7ECF;&#x5B8C;&#x6210;&#x5566;&#xFF0C;&#x5269;&#x4E0B;&#x7684;&#x5C31;&#x662F;&#x4FEE;&#x4E00;&#x4E0B;&#x8FB9;&#x8FB9;&#x89D2;&#x89D2;&#x7684;&#x4E1C;&#x897F;&#xFF0C;&#x6211;&#x4EEC;&#x60F3;&#x8FBE;&#x5230;&#x7684;&#x76EE;&#x7684;&#x662F;&#xFF1A;</p>
<p>&#x6309;&#x4E0B;&#x56DE;&#x8F66;&#xFF0C;&#x8BFB;&#x53D6;&#x4E00;&#x4E2A;&#x6BB5;&#x5B50;&#xFF0C;&#x663E;&#x793A;&#x51FA;&#x6BB5;&#x5B50;&#x7684;&#x53D1;&#x5E03;&#x4EBA;&#xFF0C;&#x53D1;&#x5E03;&#x65E5;&#x671F;&#xFF0C;&#x5185;&#x5BB9;&#x4EE5;&#x53CA;&#x70B9;&#x8D5E;&#x4E2A;&#x6570;&#x3002;</p>
<p>&#x53E6;&#x5916;&#x6211;&#x4EEC;&#x9700;&#x8981;&#x8BBE;&#x8BA1;&#x9762;&#x5411;&#x5BF9;&#x8C61;&#x6A21;&#x5F0F;&#xFF0C;&#x5F15;&#x5165;&#x7C7B;&#x548C;&#x65B9;&#x6CD5;&#xFF0C;&#x5C06;&#x4EE3;&#x7801;&#x505A;&#x4E00;&#x4E0B;&#x4F18;&#x5316;&#x548C;&#x5C01;&#x88C5;&#xFF0C;&#x6700;&#x540E;&#xFF0C;&#x6211;&#x4EEC;&#x7684;&#x4EE3;&#x7801;&#x5982;&#x4E0B;&#x6240;&#x793A;</p>
<pre><code>__author__ = &apos;CQC&apos;
# -*- coding:utf-8 -*-
import urllib
import urllib2
import re
import thread
import time

#&#x7CD7;&#x4E8B;&#x767E;&#x79D1;&#x722C;&#x866B;&#x7C7B;
class QSBK:

    #&#x521D;&#x59CB;&#x5316;&#x65B9;&#x6CD5;&#xFF0C;&#x5B9A;&#x4E49;&#x4E00;&#x4E9B;&#x53D8;&#x91CF;
    def __init__(self):
        self.pageIndex = 1
        self.user_agent = &apos;Mozilla/4.0 (compatible; MSIE 5.5; Windows NT)&apos;
        #&#x521D;&#x59CB;&#x5316;headers
        self.headers = { &apos;User-Agent&apos; : self.user_agent }
        #&#x5B58;&#x653E;&#x6BB5;&#x5B50;&#x7684;&#x53D8;&#x91CF;&#xFF0C;&#x6BCF;&#x4E00;&#x4E2A;&#x5143;&#x7D20;&#x662F;&#x6BCF;&#x4E00;&#x9875;&#x7684;&#x6BB5;&#x5B50;&#x4EEC;
        self.stories = []
        #&#x5B58;&#x653E;&#x7A0B;&#x5E8F;&#x662F;&#x5426;&#x7EE7;&#x7EED;&#x8FD0;&#x884C;&#x7684;&#x53D8;&#x91CF;
        self.enable = False
    #&#x4F20;&#x5165;&#x67D0;&#x4E00;&#x9875;&#x7684;&#x7D22;&#x5F15;&#x83B7;&#x5F97;&#x9875;&#x9762;&#x4EE3;&#x7801;
    def getPage(self,pageIndex):
        try:
            url = &apos;http://www.qiushibaike.com/hot/page/&apos; + str(pageIndex)
            #&#x6784;&#x5EFA;&#x8BF7;&#x6C42;&#x7684;request
            request = urllib2.Request(url,headers = self.headers)
            #&#x5229;&#x7528;urlopen&#x83B7;&#x53D6;&#x9875;&#x9762;&#x4EE3;&#x7801;
            response = urllib2.urlopen(request)
            #&#x5C06;&#x9875;&#x9762;&#x8F6C;&#x5316;&#x4E3A;UTF-8&#x7F16;&#x7801;
            pageCode = response.read().decode(&apos;utf-8&apos;)
            return pageCode

        except urllib2.URLError, e:
            if hasattr(e,&quot;reason&quot;):
                print u&quot;&#x8FDE;&#x63A5;&#x7CD7;&#x4E8B;&#x767E;&#x79D1;&#x5931;&#x8D25;,&#x9519;&#x8BEF;&#x539F;&#x56E0;&quot;,e.reason
                return None


    #&#x4F20;&#x5165;&#x67D0;&#x4E00;&#x9875;&#x4EE3;&#x7801;&#xFF0C;&#x8FD4;&#x56DE;&#x672C;&#x9875;&#x4E0D;&#x5E26;&#x56FE;&#x7247;&#x7684;&#x6BB5;&#x5B50;&#x5217;&#x8868;
    def getPageItems(self,pageIndex):
        pageCode = self.getPage(pageIndex)
        if not pageCode:
            print &quot;&#x9875;&#x9762;&#x52A0;&#x8F7D;&#x5931;&#x8D25;....&quot;
            return None
        pattern = re.compile(&apos;&lt;div.*?author&quot;&gt;.*?&lt;a.*?&lt;img.*?&gt;(.*?)&lt;/a&gt;.*?&lt;div.*?&apos;+
                         &apos;content&quot;&gt;(.*?)&lt;!--(.*?)--&gt;.*?&lt;/div&gt;(.*?)&lt;div class=&quot;stats.*?class=&quot;number&quot;&gt;(.*?)&lt;/i&gt;&apos;,re.S)
        items = re.findall(pattern,pageCode)
        #&#x7528;&#x6765;&#x5B58;&#x50A8;&#x6BCF;&#x9875;&#x7684;&#x6BB5;&#x5B50;&#x4EEC;
        pageStories = []
        #&#x904D;&#x5386;&#x6B63;&#x5219;&#x8868;&#x8FBE;&#x5F0F;&#x5339;&#x914D;&#x7684;&#x4FE1;&#x606F;
        for item in items:
            #&#x662F;&#x5426;&#x542B;&#x6709;&#x56FE;&#x7247;
            haveImg = re.search(&quot;img&quot;,item[3])
            #&#x5982;&#x679C;&#x4E0D;&#x542B;&#x6709;&#x56FE;&#x7247;&#xFF0C;&#x628A;&#x5B83;&#x52A0;&#x5165;list&#x4E2D;
            if not haveImg:
                replaceBR = re.compile(&apos;&lt;br/&gt;&apos;)
                text = re.sub(replaceBR,&quot;\n&quot;,item[1])
                #item[0]&#x662F;&#x4E00;&#x4E2A;&#x6BB5;&#x5B50;&#x7684;&#x53D1;&#x5E03;&#x8005;&#xFF0C;item[1]&#x662F;&#x5185;&#x5BB9;&#xFF0C;item[2]&#x662F;&#x53D1;&#x5E03;&#x65F6;&#x95F4;,item[4]&#x662F;&#x70B9;&#x8D5E;&#x6570;
                pageStories.append([item[0].strip(),text.strip(),item[2].strip(),item[4].strip()])
        return pageStories

    #&#x52A0;&#x8F7D;&#x5E76;&#x63D0;&#x53D6;&#x9875;&#x9762;&#x7684;&#x5185;&#x5BB9;&#xFF0C;&#x52A0;&#x5165;&#x5230;&#x5217;&#x8868;&#x4E2D;
    def loadPage(self):
        #&#x5982;&#x679C;&#x5F53;&#x524D;&#x672A;&#x770B;&#x7684;&#x9875;&#x6570;&#x5C11;&#x4E8E;2&#x9875;&#xFF0C;&#x5219;&#x52A0;&#x8F7D;&#x65B0;&#x4E00;&#x9875;
        if self.enable == True:
            if len(self.stories) &lt; 2:
                #&#x83B7;&#x53D6;&#x65B0;&#x4E00;&#x9875;
                pageStories = self.getPageItems(self.pageIndex)
                #&#x5C06;&#x8BE5;&#x9875;&#x7684;&#x6BB5;&#x5B50;&#x5B58;&#x653E;&#x5230;&#x5168;&#x5C40;list&#x4E2D;
                if pageStories:
                    self.stories.append(pageStories)
                    #&#x83B7;&#x53D6;&#x5B8C;&#x4E4B;&#x540E;&#x9875;&#x7801;&#x7D22;&#x5F15;&#x52A0;&#x4E00;&#xFF0C;&#x8868;&#x793A;&#x4E0B;&#x6B21;&#x8BFB;&#x53D6;&#x4E0B;&#x4E00;&#x9875;
                    self.pageIndex += 1

    #&#x8C03;&#x7528;&#x8BE5;&#x65B9;&#x6CD5;&#xFF0C;&#x6BCF;&#x6B21;&#x6572;&#x56DE;&#x8F66;&#x6253;&#x5370;&#x8F93;&#x51FA;&#x4E00;&#x4E2A;&#x6BB5;&#x5B50;
    def getOneStory(self,pageStories,page):
        #&#x904D;&#x5386;&#x4E00;&#x9875;&#x7684;&#x6BB5;&#x5B50;
        for story in pageStories:
            #&#x7B49;&#x5F85;&#x7528;&#x6237;&#x8F93;&#x5165;
            input = raw_input()
            #&#x6BCF;&#x5F53;&#x8F93;&#x5165;&#x56DE;&#x8F66;&#x4E00;&#x6B21;&#xFF0C;&#x5224;&#x65AD;&#x4E00;&#x4E0B;&#x662F;&#x5426;&#x8981;&#x52A0;&#x8F7D;&#x65B0;&#x9875;&#x9762;
            self.loadPage()
            #&#x5982;&#x679C;&#x8F93;&#x5165;Q&#x5219;&#x7A0B;&#x5E8F;&#x7ED3;&#x675F;
            if input == &quot;Q&quot;:
                self.enable = False
                return
            print u&quot;&#x7B2C;%d&#x9875;\t&#x53D1;&#x5E03;&#x4EBA;:%s\t&#x53D1;&#x5E03;&#x65F6;&#x95F4;:%s\t&#x8D5E;:%s\n%s&quot; %(page,story[0],story[2],story[3],story[1])

    #&#x5F00;&#x59CB;&#x65B9;&#x6CD5;
    def start(self):
        print u&quot;&#x6B63;&#x5728;&#x8BFB;&#x53D6;&#x7CD7;&#x4E8B;&#x767E;&#x79D1;,&#x6309;&#x56DE;&#x8F66;&#x67E5;&#x770B;&#x65B0;&#x6BB5;&#x5B50;&#xFF0C;Q&#x9000;&#x51FA;&quot;
        #&#x4F7F;&#x53D8;&#x91CF;&#x4E3A;True&#xFF0C;&#x7A0B;&#x5E8F;&#x53EF;&#x4EE5;&#x6B63;&#x5E38;&#x8FD0;&#x884C;
        self.enable = True
        #&#x5148;&#x52A0;&#x8F7D;&#x4E00;&#x9875;&#x5185;&#x5BB9;
        self.loadPage()
        #&#x5C40;&#x90E8;&#x53D8;&#x91CF;&#xFF0C;&#x63A7;&#x5236;&#x5F53;&#x524D;&#x8BFB;&#x5230;&#x4E86;&#x7B2C;&#x51E0;&#x9875;
        nowPage = 0
        while self.enable:
            if len(self.stories)&gt;0:
                #&#x4ECE;&#x5168;&#x5C40;list&#x4E2D;&#x83B7;&#x53D6;&#x4E00;&#x9875;&#x7684;&#x6BB5;&#x5B50;
                pageStories = self.stories[0]
                #&#x5F53;&#x524D;&#x8BFB;&#x5230;&#x7684;&#x9875;&#x6570;&#x52A0;&#x4E00;
                nowPage += 1
                #&#x5C06;&#x5168;&#x5C40;list&#x4E2D;&#x7B2C;&#x4E00;&#x4E2A;&#x5143;&#x7D20;&#x5220;&#x9664;&#xFF0C;&#x56E0;&#x4E3A;&#x5DF2;&#x7ECF;&#x53D6;&#x51FA;
                del self.stories[0]
                #&#x8F93;&#x51FA;&#x8BE5;&#x9875;&#x7684;&#x6BB5;&#x5B50;
                self.getOneStory(pageStories,nowPage)


spider = QSBK()
spider.start()
</code></pre><p>&#x597D;&#x5566;&#xFF0C;&#x5927;&#x5BB6;&#x6765;&#x6D4B;&#x8BD5;&#x4E00;&#x4E0B;&#x5427;&#xFF0C;&#x70B9;&#x4E00;&#x4E0B;&#x56DE;&#x8F66;&#x4F1A;&#x8F93;&#x51FA;&#x4E00;&#x4E2A;&#x6BB5;&#x5B50;&#xFF0C;&#x5305;&#x62EC;&#x53D1;&#x5E03;&#x4EBA;&#xFF0C;&#x53D1;&#x5E03;&#x65F6;&#x95F4;&#xFF0C;&#x6BB5;&#x5B50;&#x5185;&#x5BB9;&#x4EE5;&#x53CA;&#x70B9;&#x8D5E;&#x6570;&#xFF0C;&#x662F;&#x4E0D;&#x662F;&#x611F;&#x89C9;&#x723D;&#x7206;&#x4E86;&#xFF01;</p>
<p>&#x6211;&#x4EEC;&#x7B2C;&#x4E00;&#x4E2A;&#x722C;&#x866B;&#x5B9E;&#x6218;&#x9879;&#x76EE;&#x4ECB;&#x7ECD;&#x5230;&#x8FD9;&#x91CC;&#xFF0C;&#x6B22;&#x8FCE;&#x5927;&#x5BB6;&#x7EE7;&#x7EED;&#x5173;&#x6CE8;&#xFF0C;&#x5C0F;&#x4F19;&#x4F34;&#x4EEC;&#x52A0;&#x6CB9;&#xFF01;</p>

                    
                    </section>
                
                
                </div>
            </div>
        </div>

        
        <a href="../chapter2/index.html" class="navigation navigation-prev " aria-label="Previous page: 二、爬虫实战"><i class="fa fa-angle-left"></i></a>
        
        
        <a href="../chapter2/section2.html" class="navigation navigation-next " aria-label="Next page: 2. Python爬虫实战二之爬取百度贴吧帖子"><i class="fa fa-angle-right"></i></a>
        
    </div>
</div>

        
<script src="../gitbook/app.js"></script>

    
    <script src="../gitbook/plugins/gitbook-plugin-search/lunr.min.js"></script>
    

    
    <script src="../gitbook/plugins/gitbook-plugin-search/search.js"></script>
    

    
    <script src="../gitbook/plugins/gitbook-plugin-sharing/buttons.js"></script>
    

    
    <script src="../gitbook/plugins/gitbook-plugin-fontsettings/buttons.js"></script>
    

    
    <script src="../gitbook/plugins/gitbook-plugin-livereload/plugin.js"></script>
    

<script>
require(["gitbook"], function(gitbook) {
    var config = {"highlight":{},"search":{"maxIndexSize":1000000},"sharing":{"facebook":true,"twitter":true,"google":false,"weibo":false,"instapaper":false,"vk":false,"all":["facebook","google","twitter","weibo","instapaper"]},"fontsettings":{"theme":"white","family":"sans","size":2},"livereload":{}};
    gitbook.start(config);
});
</script>

        
    </body>
    
</html>
