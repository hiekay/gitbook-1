<!DOCTYPE HTML>
<html lang="en" >
    
    <head>
        
        <meta charset="UTF-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge" />
        <title>4. Python爬虫进阶四之PySpider的用法 | Python爬虫学习系列教程</title>
        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <meta name="description" content="">
        <meta name="generator" content="GitBook 2.6.7">
        
        
        <meta name="HandheldFriendly" content="true"/>
        <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
        <meta name="apple-mobile-web-app-capable" content="yes">
        <meta name="apple-mobile-web-app-status-bar-style" content="black">
        <link rel="apple-touch-icon-precomposed" sizes="152x152" href="../gitbook/images/apple-touch-icon-precomposed-152.png">
        <link rel="shortcut icon" href="../gitbook/images/favicon.ico" type="image/x-icon">
        
    <link rel="stylesheet" href="../gitbook/style.css">
    
        
        <link rel="stylesheet" href="../gitbook/plugins/gitbook-plugin-highlight/website.css">
        
    
        
        <link rel="stylesheet" href="../gitbook/plugins/gitbook-plugin-search/search.css">
        
    
        
        <link rel="stylesheet" href="../gitbook/plugins/gitbook-plugin-fontsettings/website.css">
        
    
    

        
    
    
    <link rel="next" href="../chapter4/section5.html" />
    
    
    <link rel="prev" href="../chapter4/section3.html" />
    

        
    </head>
    <body>
        
        
    <div class="book"
        data-level="4.4"
        data-chapter-title="4. Python爬虫进阶四之PySpider的用法"
        data-filepath="chapter4/section4.md"
        data-basepath=".."
        data-revision="Wed Jun 21 2017 19:16:14 GMT+0800 (中国标准时间)"
        data-innerlanguage="">
    

<div class="book-summary">
    <nav role="navigation">
        <ul class="summary">
            
            
            
            

            

            
    
        <li class="chapter " data-level="0" data-path="index.html">
            
                
                    <a href="../index.html">
                
                        <i class="fa fa-check"></i>
                        
                        Python爬虫学习系列教程
                    </a>
            
            
        </li>
    
        <li class="chapter " data-level="1" data-path="chapter1/index.html">
            
                
                    <a href="../chapter1/index.html">
                
                        <i class="fa fa-check"></i>
                        
                            <b>1.</b>
                        
                        一、爬虫入门
                    </a>
            
            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.1" data-path="chapter1/section1.html">
            
                
                    <a href="../chapter1/section1.html">
                
                        <i class="fa fa-check"></i>
                        
                            <b>1.1.</b>
                        
                        1. Python爬虫入门一之综述
                    </a>
            
            
        </li>
    
        <li class="chapter " data-level="1.2" data-path="chapter1/section2.html">
            
                
                    <a href="../chapter1/section2.html">
                
                        <i class="fa fa-check"></i>
                        
                            <b>1.2.</b>
                        
                        2. Python爬虫入门二之爬虫基础了解
                    </a>
            
            
        </li>
    
        <li class="chapter " data-level="1.3" data-path="chapter1/section3.html">
            
                
                    <a href="../chapter1/section3.html">
                
                        <i class="fa fa-check"></i>
                        
                            <b>1.3.</b>
                        
                        3. Python爬虫入门三之Urllib库的基本使用
                    </a>
            
            
        </li>
    
        <li class="chapter " data-level="1.4" data-path="chapter1/section4.html">
            
                
                    <a href="../chapter1/section4.html">
                
                        <i class="fa fa-check"></i>
                        
                            <b>1.4.</b>
                        
                        4. Python爬虫入门四之Urllib库的高级用法
                    </a>
            
            
        </li>
    
        <li class="chapter " data-level="1.5" data-path="chapter1/section5.html">
            
                
                    <a href="../chapter1/section5.html">
                
                        <i class="fa fa-check"></i>
                        
                            <b>1.5.</b>
                        
                        5. Python爬虫入门五之URLError异常处理
                    </a>
            
            
        </li>
    
        <li class="chapter " data-level="1.6" data-path="chapter1/section6.html">
            
                
                    <a href="../chapter1/section6.html">
                
                        <i class="fa fa-check"></i>
                        
                            <b>1.6.</b>
                        
                        6. Python爬虫入门六之Cookie的使用
                    </a>
            
            
        </li>
    
        <li class="chapter " data-level="1.7" data-path="chapter1/section7.html">
            
                
                    <a href="../chapter1/section7.html">
                
                        <i class="fa fa-check"></i>
                        
                            <b>1.7.</b>
                        
                        7. Python爬虫入门七之正则表达式
                    </a>
            
            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="2" data-path="chapter2/index.html">
            
                
                    <a href="../chapter2/index.html">
                
                        <i class="fa fa-check"></i>
                        
                            <b>2.</b>
                        
                        二、爬虫实战
                    </a>
            
            
            <ul class="articles">
                
    
        <li class="chapter " data-level="2.1" data-path="chapter2/section1.html">
            
                
                    <a href="../chapter2/section1.html">
                
                        <i class="fa fa-check"></i>
                        
                            <b>2.1.</b>
                        
                        1. Python爬虫实战一之爬取糗事百科段子
                    </a>
            
            
        </li>
    
        <li class="chapter " data-level="2.2" data-path="chapter2/section2.html">
            
                
                    <a href="../chapter2/section2.html">
                
                        <i class="fa fa-check"></i>
                        
                            <b>2.2.</b>
                        
                        2. Python爬虫实战二之爬取百度贴吧帖子
                    </a>
            
            
        </li>
    
        <li class="chapter " data-level="2.3" data-path="chapter2/section3.html">
            
                
                    <a href="../chapter2/section3.html">
                
                        <i class="fa fa-check"></i>
                        
                            <b>2.3.</b>
                        
                        3. Python爬虫实战三之实现山东大学无线网络掉线自动重连
                    </a>
            
            
        </li>
    
        <li class="chapter " data-level="2.4" data-path="chapter2/section4.html">
            
                
                    <a href="../chapter2/section4.html">
                
                        <i class="fa fa-check"></i>
                        
                            <b>2.4.</b>
                        
                        4. Python爬虫实战四之抓取淘宝MM照片
                    </a>
            
            
        </li>
    
        <li class="chapter " data-level="2.5" data-path="chapter2/section5.html">
            
                
                    <a href="../chapter2/section5.html">
                
                        <i class="fa fa-check"></i>
                        
                            <b>2.5.</b>
                        
                        5. Python爬虫实战五之模拟登录淘宝并获取所有订单
                    </a>
            
            
        </li>
    
        <li class="chapter " data-level="2.6" data-path="chapter2/section6.html">
            
                
                    <a href="../chapter2/section6.html">
                
                        <i class="fa fa-check"></i>
                        
                            <b>2.6.</b>
                        
                        6. Python爬虫实战六之抓取爱问知识人问题并保存至数据库
                    </a>
            
            
        </li>
    
        <li class="chapter " data-level="2.7" data-path="chapter2/section7.html">
            
                
                    <a href="../chapter2/section7.html">
                
                        <i class="fa fa-check"></i>
                        
                            <b>2.7.</b>
                        
                        7. Python爬虫实战七之计算大学本学期绩点
                    </a>
            
            
        </li>
    
        <li class="chapter " data-level="2.8" data-path="chapter2/section8.html">
            
                
                    <a href="../chapter2/section8.html">
                
                        <i class="fa fa-check"></i>
                        
                            <b>2.8.</b>
                        
                        8. Python爬虫实战八之利用Selenium抓取淘宝匿名旺旺
                    </a>
            
            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="3" data-path="chapter3/index.html">
            
                
                    <a href="../chapter3/index.html">
                
                        <i class="fa fa-check"></i>
                        
                            <b>3.</b>
                        
                        三、爬虫利器
                    </a>
            
            
            <ul class="articles">
                
    
        <li class="chapter " data-level="3.1" data-path="chapter3/section1.html">
            
                
                    <a href="../chapter3/section1.html">
                
                        <i class="fa fa-check"></i>
                        
                            <b>3.1.</b>
                        
                        1. Python爬虫利器一之Requests库的用法
                    </a>
            
            
        </li>
    
        <li class="chapter " data-level="3.2" data-path="chapter3/section2.html">
            
                
                    <a href="../chapter3/section2.html">
                
                        <i class="fa fa-check"></i>
                        
                            <b>3.2.</b>
                        
                        2. Python爬虫利器二之Beautiful Soup的用法
                    </a>
            
            
        </li>
    
        <li class="chapter " data-level="3.3" data-path="chapter3/section3.html">
            
                
                    <a href="../chapter3/section3.html">
                
                        <i class="fa fa-check"></i>
                        
                            <b>3.3.</b>
                        
                        3. Python爬虫利器三之Xpath语法与lxml库的用法
                    </a>
            
            
        </li>
    
        <li class="chapter " data-level="3.4" data-path="chapter3/section4.html">
            
                
                    <a href="../chapter3/section4.html">
                
                        <i class="fa fa-check"></i>
                        
                            <b>3.4.</b>
                        
                        4. Python爬虫利器四之PhantomJS的用法
                    </a>
            
            
        </li>
    
        <li class="chapter " data-level="3.5" data-path="chapter3/section5.html">
            
                
                    <a href="../chapter3/section5.html">
                
                        <i class="fa fa-check"></i>
                        
                            <b>3.5.</b>
                        
                        5. Python爬虫利器五之Selenium的用法
                    </a>
            
            
        </li>
    
        <li class="chapter " data-level="3.6" data-path="chapter3/section6.html">
            
                
                    <a href="../chapter3/section6.html">
                
                        <i class="fa fa-check"></i>
                        
                            <b>3.6.</b>
                        
                        6. Python爬虫利器六之PyQuery的用法
                    </a>
            
            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="4" data-path="chapter4/index.html">
            
                
                    <a href="../chapter4/index.html">
                
                        <i class="fa fa-check"></i>
                        
                            <b>4.</b>
                        
                        四、爬虫进阶
                    </a>
            
            
            <ul class="articles">
                
    
        <li class="chapter " data-level="4.1" data-path="chapter4/section1.html">
            
                
                    <a href="../chapter4/section1.html">
                
                        <i class="fa fa-check"></i>
                        
                            <b>4.1.</b>
                        
                        1. Python爬虫进阶一之爬虫框架概述
                    </a>
            
            
        </li>
    
        <li class="chapter " data-level="4.2" data-path="chapter4/section2.html">
            
                
                    <a href="../chapter4/section2.html">
                
                        <i class="fa fa-check"></i>
                        
                            <b>4.2.</b>
                        
                        2. Python爬虫进阶二之PySpider框架安装配置
                    </a>
            
            
        </li>
    
        <li class="chapter " data-level="4.3" data-path="chapter4/section3.html">
            
                
                    <a href="../chapter4/section3.html">
                
                        <i class="fa fa-check"></i>
                        
                            <b>4.3.</b>
                        
                        3. Python爬虫进阶三之爬虫框架Scrapy安装配置
                    </a>
            
            
        </li>
    
        <li class="chapter active" data-level="4.4" data-path="chapter4/section4.html">
            
                
                    <a href="../chapter4/section4.html">
                
                        <i class="fa fa-check"></i>
                        
                            <b>4.4.</b>
                        
                        4. Python爬虫进阶四之PySpider的用法
                    </a>
            
            
        </li>
    
        <li class="chapter " data-level="4.5" data-path="chapter4/section5.html">
            
                
                    <a href="../chapter4/section5.html">
                
                        <i class="fa fa-check"></i>
                        
                            <b>4.5.</b>
                        
                        5. Python爬虫进阶五之多线程的用法
                    </a>
            
            
        </li>
    
        <li class="chapter " data-level="4.6" data-path="chapter4/section6.html">
            
                
                    <a href="../chapter4/section6.html">
                
                        <i class="fa fa-check"></i>
                        
                            <b>4.6.</b>
                        
                        6. Python爬虫进阶六之多进程的用法
                    </a>
            
            
        </li>
    
        <li class="chapter " data-level="4.7" data-path="chapter4/section7.html">
            
                
                    <a href="../chapter4/section7.html">
                
                        <i class="fa fa-check"></i>
                        
                            <b>4.7.</b>
                        
                        7. Python爬虫进阶七之设置ADSL拨号服务器代理
                    </a>
            
            
        </li>
    

            </ul>
            
        </li>
    


            
            <li class="divider"></li>
            <li>
                <a href="https://www.gitbook.com" target="blank" class="gitbook-link">
                    Published with GitBook
                </a>
            </li>
            
        </ul>
    </nav>
</div>

    <div class="book-body">
        <div class="body-inner">
            <div class="book-header" role="navigation">
    <!-- Actions Left -->
    

    <!-- Title -->
    <h1>
        <i class="fa fa-circle-o-notch fa-spin"></i>
        <a href="../" >Python爬虫学习系列教程</a>
    </h1>
</div>

            <div class="page-wrapper" tabindex="-1" role="main">
                <div class="page-inner">
                
                
                    <section class="normal" id="section-">
                    
                        <h1 id="python&#x722C;&#x866B;&#x8FDB;&#x9636;&#x56DB;&#x4E4B;pyspider&#x7684;&#x7528;&#x6CD5;">Python&#x722C;&#x866B;&#x8FDB;&#x9636;&#x56DB;&#x4E4B;PySpider&#x7684;&#x7528;&#x6CD5;</h1>
<h2 id="&#x5BA1;&#x65F6;&#x5EA6;&#x52BF;">&#x5BA1;&#x65F6;&#x5EA6;&#x52BF;</h2>
<p>PySpider &#x662F;&#x4E00;&#x4E2A;&#x6211;&#x4E2A;&#x4EBA;&#x8BA4;&#x4E3A;&#x975E;&#x5E38;&#x65B9;&#x4FBF;&#x5E76;&#x4E14;&#x529F;&#x80FD;&#x5F3A;&#x5927;&#x7684;&#x722C;&#x866B;&#x6846;&#x67B6;&#xFF0C;&#x652F;&#x6301;&#x591A;&#x7EBF;&#x7A0B;&#x722C;&#x53D6;&#x3001;JS&#x52A8;&#x6001;&#x89E3;&#x6790;&#xFF0C;&#x63D0;&#x4F9B;&#x4E86;&#x53EF;&#x64CD;&#x4F5C;&#x754C;&#x9762;&#x3001;&#x51FA;&#x9519;&#x91CD;&#x8BD5;&#x3001;&#x5B9A;&#x65F6;&#x722C;&#x53D6;&#x7B49;&#x7B49;&#x7684;&#x529F;&#x80FD;&#xFF0C;&#x4F7F;&#x7528;&#x975E;&#x5E38;&#x4EBA;&#x6027;&#x5316;&#x3002;</p>
<p>&#x672C;&#x7BC7;&#x5185;&#x5BB9;&#x901A;&#x8FC7;&#x8DDF;&#x6211;&#x505A;&#x4E00;&#x4E2A;&#x597D;&#x73A9;&#x7684; PySpider &#x9879;&#x76EE;&#xFF0C;&#x6765;&#x7406;&#x89E3; PySpider &#x7684;&#x8FD0;&#x884C;&#x6D41;&#x7A0B;&#x3002;</p>
<h2 id="&#x62DB;&#x5175;&#x4E70;&#x9A6C;">&#x62DB;&#x5175;&#x4E70;&#x9A6C;</h2>
<p>&#x5177;&#x4F53;&#x7684;<a href="http://cuiqingcai.com/2443.html" target="_blank">&#x5B89;&#x88C5;&#x8FC7;&#x7A0B;</a>&#x8BF7;&#x67E5;&#x770B;&#x672C;&#x8282;&#x8BB2;&#x8FF0;</p>
<p>&#x55EF;&#xFF0C;&#x5B89;&#x88C5;&#x597D;&#x4E86;&#x4E4B;&#x540E;&#x5C31;&#x4E0E;&#x6211;&#x5927;&#x5E72;&#x4E00;&#x756A;&#x5427;&#x3002;</p>
<h2 id="&#x9E3F;&#x9E44;&#x4E4B;&#x5FD7;">&#x9E3F;&#x9E44;&#x4E4B;&#x5FD7;</h2>
<p>&#x6211;&#x4E4B;&#x524D;&#x5199;&#x8FC7;&#x7684;&#x4E00;&#x7BC7;&#x6587;&#x7AE0;<a href="http://cuiqingcai.com/1001.html" target="_blank">&#x6293;&#x53D6;&#x6DD8;&#x5B9D;MM&#x7167;&#x7247;</a></p>
<p>&#x7531;&#x4E8E;&#x7F51;&#x9875;&#x6539;&#x7248;&#xFF0C;&#x722C;&#x53D6;&#x8FC7;&#x7A0B;&#x4E2D;&#x9700;&#x8981;&#x7684; URL &#x9700;&#x8981; JS &#x52A8;&#x6001;&#x89E3;&#x6790;&#x751F;&#x6210;&#xFF0C;&#x6240;&#x4EE5;&#x4E4B;&#x524D;&#x7528;&#x7684; urllib2 &#x4E0D;&#x80FD;&#x7EE7;&#x7EED;&#x4F7F;&#x7528;&#x4E86;&#xFF0C;&#x5728;&#x8FD9;&#x91CC;&#x6211;&#x4EEC;&#x5229;&#x7528; PySpider &#x91CD;&#x65B0;&#x5B9E;&#x73B0;&#x4E00;&#x4E0B;&#x3002;</p>
<p>&#x6240;&#x4EE5;&#x73B0;&#x5728;&#x6211;&#x4EEC;&#x9700;&#x8981;&#x505A;&#x7684;&#x662F;&#x6293;&#x53D6;&#x6DD8;&#x5B9D;MM&#x7684;&#x4E2A;&#x4EBA;&#x4FE1;&#x606F;&#x548C;&#x56FE;&#x7247;&#x5B58;&#x50A8;&#x5230;&#x672C;&#x5730;&#x3002;</p>
<h2 id="&#x5BA1;&#x65F6;&#x5EA6;&#x52BF;">&#x5BA1;&#x65F6;&#x5EA6;&#x52BF;</h2>
<p>&#x722C;&#x53D6;&#x76EE;&#x6807;&#x7F51;&#x7AD9;&#xFF1A;<a href="https://mm.taobao.com/json/request_top_list.htm?page=1&#xFF0C;&#x5927;&#x5BB6;&#x6253;&#x5F00;&#x4E4B;&#x540E;&#x53EF;&#x4EE5;&#x770B;&#x5230;&#x8BB8;&#x591A;&#x6DD8;&#x5B9D;MM&#x7684;&#x5217;&#x8868;&#x3002;" target="_blank">https://mm.taobao.com/json/request_top_list.htm?page=1&#xFF0C;&#x5927;&#x5BB6;&#x6253;&#x5F00;&#x4E4B;&#x540E;&#x53EF;&#x4EE5;&#x770B;&#x5230;&#x8BB8;&#x591A;&#x6DD8;&#x5B9D;MM&#x7684;&#x5217;&#x8868;&#x3002;</a></p>
<p>&#x5217;&#x8868;&#x6709;&#x591A;&#x5C11;&#xFF1F;</p>
<p><a href="https://mm.taobao.com/json/request_top_list.htm?page=10000&#xFF0C;&#x7B2C;10000&#x9875;&#x90FD;&#x6709;&#xFF0C;&#x770B;&#x4F60;&#x60F3;&#x8981;&#x591A;&#x5C11;&#x3002;&#x6211;&#x4EC0;&#x4E48;&#x4E5F;&#x4E0D;&#x77E5;&#x9053;&#x3002;" target="_blank">https://mm.taobao.com/json/request_top_list.htm?page=10000&#xFF0C;&#x7B2C;10000&#x9875;&#x90FD;&#x6709;&#xFF0C;&#x770B;&#x4F60;&#x60F3;&#x8981;&#x591A;&#x5C11;&#x3002;&#x6211;&#x4EC0;&#x4E48;&#x4E5F;&#x4E0D;&#x77E5;&#x9053;&#x3002;</a></p>
<p>&#x968F;&#x673A;&#x70B9;&#x51FB;&#x4E00;&#x4F4D; MM &#x7684;&#x59D3;&#x540D;&#xFF0C;&#x53EF;&#x4EE5;&#x770B;&#x5230;&#x5979;&#x7684;&#x57FA;&#x672C;&#x8D44;&#x6599;&#x3002;</p>
<p><img src="../image/chapter4/section4-1.png" alt=""></p>
<p>&#x53EF;&#x4EE5;&#x770B;&#x5230;&#x56FE;&#x4E2D;&#x6709;&#x4E00;&#x4E2A;&#x4E2A;&#x6027;&#x57DF;&#x540D;&#xFF0C;&#x6211;&#x4EEC;&#x590D;&#x5236;&#x5230;&#x6D4F;&#x89C8;&#x5668;&#x6253;&#x5F00;&#x3002;mm.taobao.com/tyy6160</p>
<p><img src="../image/chapter4/section4-2.png" alt=""></p>
<p>&#x55EF;&#xFF0C;&#x5F80;&#x4E0B;&#x62D6;&#xFF0C;&#x6D77;&#x91CF;&#x7684; MM &#x56FE;&#x7247;&#x90FD;&#x5728;&#x8FD9;&#x91CC;&#x4E86;&#xFF0C;&#x600E;&#x4E48;&#x529E;&#x4F60;&#x61C2;&#x5F97;&#xFF0C;&#x6211;&#x4EEC;&#x8981;&#x628A;&#x5979;&#x4EEC;&#x7684;&#x7167;&#x7247;&#x548C;&#x4E2A;&#x4EBA;&#x4FE1;&#x606F;&#x90FD;&#x5B58;&#x4E0B;&#x6765;&#x3002;</p>
<p>P.S. &#x6CE8;&#x610F;&#x56FE;&#x4E2D;&#x8FDB;&#x5EA6;&#x6761;&#xFF01;&#x4F60;&#x731C;&#x6709;&#x591A;&#x5C11;&#x56FE;&#x7247;&#xFF5E;</p>
<h2 id="&#x5229;&#x5251;&#x51FA;&#x9798;">&#x5229;&#x5251;&#x51FA;&#x9798;</h2>
<p>&#x5B89;&#x88C5;&#x6210;&#x529F;&#x4E4B;&#x540E;&#xFF0C;&#x8DDF;&#x6211;&#x4E00;&#x6B65;&#x6B65;&#x5730;&#x5B8C;&#x6210;&#x4E00;&#x4E2A;&#x7F51;&#x7AD9;&#x7684;&#x6293;&#x53D6;&#xFF0C;&#x4F60;&#x5C31;&#x4F1A;&#x660E;&#x767D; PySpider &#x7684;&#x57FA;&#x672C;&#x7528;&#x6CD5;&#x4E86;&#x3002;</p>
<p>&#x547D;&#x4EE4;&#x884C;&#x4E0B;&#x6267;&#x884C;</p>
<pre><code>pyspider all
</code></pre><p>&#x8FD9;&#x53E5;&#x547D;&#x4EE4;&#x7684;&#x610F;&#x601D;&#x662F;&#xFF0C;&#x8FD0;&#x884C; pyspider &#x5E76; &#x542F;&#x52A8;&#x5B83;&#x7684;&#x6240;&#x6709;&#x7EC4;&#x4EF6;&#x3002;</p>
<p><img src="../image/chapter4/section4-3.png" alt=""></p>
<p>&#x53EF;&#x4EE5;&#x53D1;&#x73B0;&#x7A0B;&#x5E8F;&#x5DF2;&#x7ECF;&#x6B63;&#x5E38;&#x542F;&#x52A8;&#xFF0C;&#x5E76;&#x5728; 5000 &#x8FD9;&#x4E2A;&#x7AEF;&#x53E3;&#x8FD0;&#x884C;&#x3002;</p>
<h2 id="&#x4E00;&#x89E6;&#x5373;&#x53D1;">&#x4E00;&#x89E6;&#x5373;&#x53D1;</h2>
<p>&#x63A5;&#x4E0B;&#x6765;&#x5728;&#x6D4F;&#x89C8;&#x5668;&#x4E2D;&#x8F93;&#x5165; <a href="http://localhost:5000&#xFF0C;&#x53EF;&#x4EE5;&#x770B;&#x5230;" target="_blank">http://localhost:5000&#xFF0C;&#x53EF;&#x4EE5;&#x770B;&#x5230;</a> PySpider &#x7684;&#x4E3B;&#x754C;&#x9762;&#xFF0C;&#x70B9;&#x51FB;&#x53F3;&#x4E0B;&#x89D2;&#x7684; Create&#xFF0C;&#x547D;&#x540D;&#x4E3A; taobaomm&#xFF0C;&#x5F53;&#x7136;&#x540D;&#x79F0;&#x4F60;&#x53EF;&#x4EE5;&#x968F;&#x610F;&#x53D6;&#xFF0C;&#x7EE7;&#x7EED;&#x70B9;&#x51FB; Create&#x3002;</p>
<p><img src="../image/chapter4/section4-4.png" alt=""></p>
<p>&#x8FD9;&#x6837;&#x6211;&#x4EEC;&#x4F1A;&#x8FDB;&#x5165;&#x5230;&#x4E00;&#x4E2A;&#x722C;&#x53D6;&#x64CD;&#x4F5C;&#x7684;&#x9875;&#x9762;&#x3002;</p>
<p><img src="../image/chapter4/section4-5.png" alt=""></p>
<p>&#x6574;&#x4E2A;&#x9875;&#x9762;&#x5206;&#x4E3A;&#x4E24;&#x680F;&#xFF0C;&#x5DE6;&#x8FB9;&#x662F;&#x722C;&#x53D6;&#x9875;&#x9762;&#x9884;&#x89C8;&#x533A;&#x57DF;&#xFF0C;&#x53F3;&#x8FB9;&#x662F;&#x4EE3;&#x7801;&#x7F16;&#x5199;&#x533A;&#x57DF;&#x3002;&#x4E0B;&#x9762;&#x5BF9;&#x533A;&#x5757;&#x8FDB;&#x884C;&#x8BF4;&#x660E;&#xFF1A;</p>
<p>&#x5DE6;&#x4FA7;&#x7EFF;&#x8272;&#x533A;&#x57DF;&#xFF1A;&#x8FD9;&#x4E2A;&#x8BF7;&#x6C42;&#x5BF9;&#x5E94;&#x7684; JSON &#x53D8;&#x91CF;&#xFF0C;&#x5728; PySpider &#x4E2D;&#xFF0C;&#x5176;&#x5B9E;&#x6BCF;&#x4E2A;&#x8BF7;&#x6C42;&#x90FD;&#x6709;&#x4E0E;&#x4E4B;&#x5BF9;&#x5E94;&#x7684; JSON &#x53D8;&#x91CF;&#xFF0C;&#x5305;&#x62EC;&#x56DE;&#x8C03;&#x51FD;&#x6570;&#xFF0C;&#x65B9;&#x6CD5;&#x540D;&#xFF0C;&#x8BF7;&#x6C42;&#x94FE;&#x63A5;&#xFF0C;&#x8BF7;&#x6C42;&#x6570;&#x636E;&#x7B49;&#x7B49;&#x3002;</p>
<p>&#x7EFF;&#x8272;&#x533A;&#x57DF;&#x53F3;&#x4E0A;&#x89D2;Run&#xFF1A;&#x70B9;&#x51FB;&#x53F3;&#x4E0A;&#x89D2;&#x7684; run &#x6309;&#x94AE;&#xFF0C;&#x5C31;&#x4F1A;&#x6267;&#x884C;&#x8FD9;&#x4E2A;&#x8BF7;&#x6C42;&#xFF0C;&#x53EF;&#x4EE5;&#x5728;&#x5DE6;&#x8FB9;&#x7684;&#x767D;&#x8272;&#x533A;&#x57DF;&#x51FA;&#x73B0;&#x8BF7;&#x6C42;&#x7684;&#x7ED3;&#x679C;&#x3002;</p>
<p>&#x5DE6;&#x4FA7; enable css selector helper: &#x6293;&#x53D6;&#x9875;&#x9762;&#x4E4B;&#x540E;&#xFF0C;&#x70B9;&#x51FB;&#x6B64;&#x6309;&#x94AE;&#xFF0C;&#x53EF;&#x4EE5;&#x65B9;&#x4FBF;&#x5730;&#x83B7;&#x53D6;&#x9875;&#x9762;&#x4E2D;&#x67D0;&#x4E2A;&#x5143;&#x7D20;&#x7684; CSS &#x9009;&#x62E9;&#x5668;&#x3002;</p>
<p>&#x5DE6;&#x4FA7; web: &#x5373;&#x6293;&#x53D6;&#x7684;&#x9875;&#x9762;&#x7684;&#x5B9E;&#x65F6;&#x9884;&#x89C8;&#x56FE;&#x3002;</p>
<p>&#x5DE6;&#x4FA7; html: &#x6293;&#x53D6;&#x9875;&#x9762;&#x7684; HTML &#x4EE3;&#x7801;&#x3002;</p>
<p>&#x5DE6;&#x4FA7; follows: &#x5982;&#x679C;&#x5F53;&#x524D;&#x6293;&#x53D6;&#x65B9;&#x6CD5;&#x4E2D;&#x53C8;&#x65B0;&#x5EFA;&#x4E86;&#x722C;&#x53D6;&#x8BF7;&#x6C42;&#xFF0C;&#x90A3;&#x4E48;&#x63A5;&#x4E0B;&#x6765;&#x7684;&#x8BF7;&#x6C42;&#x5C31;&#x4F1A;&#x51FA;&#x73B0;&#x5728; follows &#x91CC;&#x3002;</p>
<p>&#x5DE6;&#x4FA7; messages: &#x722C;&#x53D6;&#x8FC7;&#x7A0B;&#x4E2D;&#x8F93;&#x51FA;&#x7684;&#x4E00;&#x4E9B;&#x4FE1;&#x606F;&#x3002;</p>
<p>&#x53F3;&#x4FA7;&#x4EE3;&#x7801;&#x533A;&#x57DF;: &#x4F60;&#x53EF;&#x4EE5;&#x5728;&#x53F3;&#x4FA7;&#x533A;&#x57DF;&#x4E66;&#x5199;&#x4EE3;&#x7801;&#xFF0C;&#x5E76;&#x70B9;&#x51FB;&#x53F3;&#x4E0A;&#x89D2;&#x7684; Save &#x6309;&#x94AE;&#x4FDD;&#x5B58;&#x3002;</p>
<p>&#x53F3;&#x4FA7; WebDAV Mode: &#x6253;&#x5F00;&#x8C03;&#x8BD5;&#x6A21;&#x5F0F;&#xFF0C;&#x5DE6;&#x4FA7;&#x6700;&#x5927;&#x5316;&#xFF0C;&#x4FBF;&#x4E8E;&#x89C2;&#x5BDF;&#x8C03;&#x8BD5;&#x3002;</p>
<h2 id="&#x4E58;&#x80DC;&#x8FFD;&#x51FB;">&#x4E58;&#x80DC;&#x8FFD;&#x51FB;</h2>
<p>&#x4F9D;&#x7136;&#x662F;&#x4E0A;&#x4E00;&#x8282;&#x7684;&#x90A3;&#x4E2A;&#x7F51;&#x5740;&#xFF0C;<a href="https://mm.taobao.com/json/request_top_list.htm?page=1&#xFF0C;&#x5176;&#x4E2D;" target="_blank">https://mm.taobao.com/json/request_top_list.htm?page=1&#xFF0C;&#x5176;&#x4E2D;</a> page &#x53C2;&#x6570;&#x4EE3;&#x8868;&#x9875;&#x7801;&#x3002;&#x6240;&#x4EE5;&#x6211;&#x4EEC;&#x6682;&#x65F6;&#x6293;&#x53D6;&#x524D; 30 &#x9875;&#x3002;&#x9875;&#x7801;&#x5230;&#x6700;&#x540E;&#x53EF;&#x4EE5;&#x968F;&#x610F;&#x8C03;&#x6574;&#x3002;</p>
<p>&#x9996;&#x5148;&#x6211;&#x4EEC;&#x5B9A;&#x4E49;&#x57FA;&#x5730;&#x5740;&#xFF0C;&#x7136;&#x540E;&#x5B9A;&#x4E49;&#x722C;&#x53D6;&#x7684;&#x9875;&#x7801;&#x548C;&#x603B;&#x9875;&#x7801;&#x3002;</p>
<pre><code>from pyspider.libs.base_handler import *


class Handler(BaseHandler):
    crawl_config = {
    }

    def __init__(self):
        self.base_url = &apos;https://mm.taobao.com/json/request_top_list.htm?page=&apos;
        self.page_num = 1
        self.total_num = 30

    @every(minutes=24 * 60)
    def on_start(self):
        while self.page_num &lt;= self.total_num:
            url = self.base_url + str(self.page_num)
            print url
            self.crawl(url, callback=self.index_page)
            self.page_num += 1

    @config(age=10 * 24 * 60 * 60)
    def index_page(self, response):
        for each in response.doc(&apos;a[href^=&quot;http&quot;]&apos;).items():
            self.crawl(each.attr.href, callback=self.detail_page)

    @config(priority=2)
    def detail_page(self, response):
        return {
            &quot;url&quot;: response.url,
            &quot;title&quot;: response.doc(&apos;title&apos;).text(),
        }
</code></pre><p>&#x70B9;&#x51FB; save &#x4FDD;&#x5B58;&#x4EE3;&#x7801;&#xFF0C;&#x7136;&#x540E;&#x70B9;&#x51FB;&#x5DE6;&#x8FB9;&#x7684; run&#xFF0C;&#x8FD0;&#x884C;&#x4EE3;&#x7801;&#x3002;</p>
<p><img src="../image/chapter4/section4-6.png" alt=""></p>
<p>&#x8FD0;&#x884C;&#x540E;&#x6211;&#x4EEC;&#x4F1A;&#x53D1;&#x73B0; follows &#x51FA;&#x73B0;&#x4E86; 30 &#x8FD9;&#x4E2A;&#x6570;&#x5B57;&#xFF0C;&#x8BF4;&#x660E;&#x6211;&#x4EEC;&#x63A5;&#x4E0B;&#x6765;&#x6709; 30 &#x4E2A;&#x65B0;&#x8BF7;&#x6C42;&#xFF0C;&#x70B9;&#x51FB;&#x53EF;&#x67E5;&#x770B;&#x6240;&#x6709;&#x722C;&#x53D6;&#x5217;&#x8868;&#x3002;&#x53E6;&#x5916;&#x63A7;&#x5236;&#x53F0;&#x4E5F;&#x6709;&#x8F93;&#x51FA;&#xFF0C;&#x5C06;&#x6240;&#x6709;&#x8981;&#x722C;&#x53D6;&#x7684; URL &#x6253;&#x5370;&#x4E86;&#x51FA;&#x6765;&#x3002;</p>
<p>&#x7136;&#x540E;&#x6211;&#x4EEC;&#x70B9;&#x51FB;&#x5DE6;&#x4FA7;&#x4EFB;&#x610F;&#x4E00;&#x4E2A;&#x7EFF;&#x8272;&#x7BAD;&#x5934;&#xFF0C;&#x53EF;&#x4EE5;&#x7EE7;&#x7EED;&#x722C;&#x53D6;&#x8FD9;&#x4E2A;&#x9875;&#x9762;&#x3002;&#x4F8B;&#x5982;&#x70B9;&#x51FB;&#x7B2C;&#x4E00;&#x4E2A; URL&#xFF0C;&#x6765;&#x722C;&#x53D6;&#x8FD9;&#x4E2A; URL</p>
<p><img src="../image/chapter4/section4-7.png" alt=""></p>
<p>&#x70B9;&#x51FB;&#x4E4B;&#x540E;&#xFF0C;&#x518D;&#x67E5;&#x770B;&#x4E0B;&#x65B9;&#x7684; web &#x9875;&#x9762;&#xFF0C;&#x53EF;&#x4EE5;&#x9884;&#x89C8;&#x5B9E;&#x65F6;&#x9875;&#x9762;&#xFF0C;&#x8FD9;&#x4E2A;&#x9875;&#x9762;&#x88AB;&#x6211;&#x4EEC;&#x722C;&#x53D6;&#x4E86;&#x4E0B;&#x6765;&#xFF0C;&#x5E76;&#x4E14;&#x56DE;&#x8C03;&#x5230; index_page &#x51FD;&#x6570;&#x6765;&#x5904;&#x7406;&#xFF0C;&#x76EE;&#x524D; index_page &#x51FD;&#x6570;&#x6211;&#x4EEC;&#x8FD8;&#x6CA1;&#x6709;&#x5904;&#x7406;&#xFF0C;&#x6240;&#x4EE5;&#x662F;&#x7EE7;&#x7EED;&#x6784;&#x4EF6;&#x4E86;&#x6240;&#x6709;&#x7684;&#x94FE;&#x63A5;&#x8BF7;&#x6C42;&#x3002;</p>
<p><img src="../image/chapter4/section4-8.png" alt=""></p>
<p>&#x597D;&#xFF0C;&#x63A5;&#x4E0B;&#x6765;&#x6211;&#x4EEC;&#x600E;&#x4E48;&#x529E;&#xFF1F;&#x5F53;&#x7136;&#x662F;&#x8FDB;&#x5165;&#x5230; MM &#x5230;&#x4E2A;&#x4EBA;&#x9875;&#x9762;&#x53BB;&#x722C;&#x53D6;&#x4E86;&#x3002;</p>
<h2 id="&#x5982;&#x706B;&#x5982;&#x837C;">&#x5982;&#x706B;&#x5982;&#x837C;</h2>
<p>&#x722C;&#x53D6;&#x5230;&#x4E86; MM &#x7684;&#x5217;&#x8868;&#xFF0C;&#x63A5;&#x4E0B;&#x6765;&#x5C31;&#x8981;&#x8FDB;&#x5165;&#x5230; MM &#x8BE6;&#x60C5;&#x9875;&#x4E86;&#xFF0C;&#x4FEE;&#x6539; index_page &#x65B9;&#x6CD5;&#x3002;</p>
<pre><code>def index_page(self, response):
    for each in response.doc(&apos;.lady-name&apos;).items():
        self.crawl(each.attr.href, callback=self.detail_page)
</code></pre><p>&#x5176;&#x4E2D; response &#x5C31;&#x662F;&#x521A;&#x624D;&#x722C;&#x53D6;&#x7684;&#x5217;&#x8868;&#x9875;&#xFF0C;response &#x5176;&#x5B9E;&#x5C31;&#x76F8;&#x5F53;&#x4E8E;&#x5217;&#x8868;&#x9875;&#x7684; html &#x4EE3;&#x7801;&#xFF0C;&#x5229;&#x7528; doc &#x51FD;&#x6570;&#xFF0C;&#x5176;&#x5B9E;&#x662F;&#x8C03;&#x7528;&#x4E86; PyQuery&#xFF0C;&#x7528; CSS &#x9009;&#x62E9;&#x5668;&#x5F97;&#x5230;&#x6BCF;&#x4E00;&#x4E2A;MM&#x7684;&#x94FE;&#x63A5;&#xFF0C;&#x7136;&#x540E;&#x91CD;&#x65B0;&#x53D1;&#x8D77;&#x65B0;&#x7684;&#x8BF7;&#x6C42;&#x3002;</p>
<p>&#x6BD4;&#x5982;&#xFF0C;&#x6211;&#x4EEC;&#x8FD9;&#x91CC;&#x62FF;&#x5230;&#x7684; each.attr.href &#x53EF;&#x80FD;&#x662F; mm.taobao.com/self/model_card.htm?user_id=687471686&#xFF0C;&#x5728;&#x8FD9;&#x91CC;&#x7EE7;&#x7EED;&#x8C03;&#x7528;&#x4E86; crawl &#x65B9;&#x6CD5;&#xFF0C;&#x4EE3;&#x8868;&#x7EE7;&#x7EED;&#x6293;&#x53D6;&#x8FD9;&#x4E2A;&#x94FE;&#x63A5;&#x7684;&#x8BE6;&#x60C5;&#x3002;</p>
<pre><code>self.crawl(each.attr.href, callback=self.detail_page)
</code></pre><p>&#x7136;&#x540E;&#x56DE;&#x8C03;&#x51FD;&#x6570;&#x5C31;&#x662F; detail_page&#xFF0C;&#x722C;&#x53D6;&#x7684;&#x7ED3;&#x679C;&#x4F1A;&#x4F5C;&#x4E3A; response &#x53D8;&#x91CF;&#x4F20;&#x8FC7;&#x53BB;&#x3002;detail_page &#x63A5;&#x5230;&#x8FD9;&#x4E2A;&#x53D8;&#x91CF;&#x7EE7;&#x7EED;&#x4E0B;&#x9762;&#x7684;&#x5206;&#x6790;&#x3002;</p>
<p><img src="../image/chapter4/section4-9.png" alt=""></p>
<p>&#x597D;&#xFF0C;&#x6211;&#x4EEC;&#x7EE7;&#x7EED;&#x70B9;&#x51FB; run &#x6309;&#x94AE;&#xFF0C;&#x5F00;&#x59CB;&#x4E0B;&#x4E00;&#x4E2A;&#x9875;&#x9762;&#x7684;&#x722C;&#x53D6;&#x3002;&#x5F97;&#x5230;&#x7684;&#x7ED3;&#x679C;&#x662F;&#x8FD9;&#x6837;&#x7684;&#x3002;</p>
<p><img src="../image/chapter4/section4-10.png" alt=""></p>
<p>&#x54E6;&#xFF0C;&#x6709;&#x4E9B;&#x9875;&#x9762;&#x6CA1;&#x6709;&#x52A0;&#x8F7D;&#x51FA;&#x6765;&#xFF0C;&#x8FD9;&#x662F;&#x4E3A;&#x4EC0;&#x4E48;&#xFF1F;</p>
<p>&#x5728;&#x4E4B;&#x524D;&#x7684;&#x6587;&#x7AE0;&#x8BF4;&#x8FC7;&#xFF0C;&#x8FD9;&#x4E2A;&#x9875;&#x9762;&#x6BD4;&#x8F83;&#x7279;&#x6B8A;&#xFF0C;&#x53F3;&#x8FB9;&#x7684;&#x9875;&#x9762;&#x4F7F;&#x7528; JS &#x6E32;&#x67D3;&#x751F;&#x6210;&#x7684;&#xFF0C;&#x800C;&#x666E;&#x901A;&#x7684;&#x6293;&#x53D6;&#x662F;&#x4E0D;&#x80FD;&#x5F97;&#x5230; JS &#x6E32;&#x67D3;&#x540E;&#x7684;&#x9875;&#x9762;&#x7684;&#xFF0C;&#x8FD9;&#x53EF;&#x9EBB;&#x70E6;&#x4E86;&#x3002;</p>
<p>&#x7136;&#x800C;&#xFF0C;&#x5E78;&#x8FD0;&#x7684;&#x662F;&#xFF0C;PySpider &#x63D0;&#x4F9B;&#x4E86;&#x52A8;&#x6001;&#x89E3;&#x6790; JS &#x7684;&#x673A;&#x5236;&#x3002;</p>
<p>&#x53CB;&#x60C5;&#x63D0;&#x793A;&#xFF1A;&#x53EF;&#x80FD;&#x6709;&#x7684;&#x5C0F;&#x4F19;&#x4F34;&#x4E0D;&#x77E5;&#x9053; PhantomJS&#xFF0C;&#x53EF;&#x4EE5;&#x53C2;&#x8003;<a href="http://cuiqingcai.com/2599.html" target="_blank">&#x722C;&#x866B;JS&#x52A8;&#x6001;&#x89E3;&#x6790;</a></p>
<p>&#x56E0;&#x4E3A;&#x6211;&#x4EEC;&#x5728;&#x524D;&#x9762;&#x88C5;&#x597D;&#x4E86; PhantomJS&#xFF0C;&#x6240;&#x4EE5;&#xFF0C;&#x8FD9;&#x65F6;&#x5019;&#x5C31;&#x8F6E;&#x5230;&#x5B83;&#x6765;&#x51FA;&#x573A;&#x4E86;&#x3002;&#x5728;&#x6700;&#x5F00;&#x59CB;&#x8FD0;&#x884C; PySpider &#x7684;&#x65F6;&#x5019;&#xFF0C;&#x4F7F;&#x7528;&#x4E86;pyspider all&#x547D;&#x4EE4;&#xFF0C;&#x8FD9;&#x4E2A;&#x547D;&#x4EE4;&#x662F;&#x628A; PySpider &#x6240;&#x6709;&#x7684;&#x7EC4;&#x4EF6;&#x542F;&#x52A8;&#x8D77;&#x6765;&#xFF0C;&#x5176;&#x4E2D;&#x4E5F;&#x5305;&#x62EC; PhantomJS&#x3002;</p>
<p>&#x6240;&#x4EE5;&#x6211;&#x4EEC;&#x4EE3;&#x7801;&#x600E;&#x4E48;&#x6539;&#x5462;&#xFF1F;&#x5F88;&#x7B80;&#x5355;&#x3002;</p>
<pre><code>def index_page(self, response):
    for each in response.doc(&apos;.lady-name&apos;).items():
        self.crawl(each.attr.href, callback=self.detail_page, fetch_type=&apos;js&apos;)
</code></pre><p>&#x53EA;&#x662F;&#x7B80;&#x5355;&#x5730;&#x52A0;&#x4E86;&#x4E00;&#x4E2A; fetch_type=&#x2019;js&#x2019;&#xFF0C;&#x70B9;&#x51FB;&#x7EFF;&#x8272;&#x7684;&#x8FD4;&#x56DE;&#x7BAD;&#x5934;&#xFF0C;&#x91CD;&#x65B0;&#x8FD0;&#x884C;&#x4E00;&#x4E0B;&#x3002;</p>
<p>&#x53EF;&#x4EE5;&#x53D1;&#x73B0;&#xFF0C;&#x9875;&#x9762;&#x5DF2;&#x7ECF;&#x88AB;&#x6211;&#x4EEC;&#x6210;&#x529F;&#x52A0;&#x8F7D;&#x51FA;&#x6765;&#x4E86;&#xFF0C;&#x7B80;&#x76F4;&#x4E0D;&#x80FD;&#x66F4;&#x5E05;&#xFF01;</p>
<p><img src="../image/chapter4/section4-11.png" alt=""></p>
<p>&#x770B;&#x4E0B;&#x9762;&#x7684;&#x4E2A;&#x6027;&#x57DF;&#x540D;&#xFF0C;&#x6240;&#x6709;&#x6211;&#x4EEC;&#x9700;&#x8981;&#x7684; MM &#x56FE;&#x7247;&#x90FD;&#x5728;&#x90A3;&#x91CC;&#x9762;&#x4E86;&#xFF0C;&#x6240;&#x4EE5;&#x6211;&#x4EEC;&#x9700;&#x8981;&#x7EE7;&#x7EED;&#x6293;&#x53D6;&#x8FD9;&#x4E2A;&#x9875;&#x9762;&#x3002;</p>
<h2 id="&#x80DC;&#x5229;&#x5728;&#x671B;">&#x80DC;&#x5229;&#x5728;&#x671B;</h2>
<p>&#x597D;&#xFF0C;&#x7EE7;&#x7EED;&#x4FEE;&#x6539; detail_page &#x65B9;&#x6CD5;&#xFF0C;&#x7136;&#x540E;&#x589E;&#x52A0;&#x4E00;&#x4E2A; domain_page &#x65B9;&#x6CD5;&#xFF0C;&#x7528;&#x6765;&#x5904;&#x7406;&#x6BCF;&#x4E2A; MM &#x7684;&#x4E2A;&#x6027;&#x57DF;&#x540D;&#x3002;</p>
<pre><code>def detail_page(self, response):
    domain = &apos;https:&apos; + response.doc(&apos;.mm-p-domain-info li &gt; span&apos;).text()
    print domain
    self.crawl(domain, callback=self.domain_page)

def domain_page(self, response):
    pass
</code></pre><p>&#x597D;&#xFF0C;&#x7EE7;&#x7EED;&#x91CD;&#x65B0; run&#xFF0C;&#x9884;&#x89C8;&#x4E00;&#x4E0B;&#x9875;&#x9762;&#xFF0C;&#x7EC8;&#x4E8E;&#xFF0C;&#x6211;&#x4EEC;&#x770B;&#x5230;&#x4E86; MM &#x7684;&#x6240;&#x6709;&#x56FE;&#x7247;&#x3002;</p>
<p><img src="../image/chapter4/section4-12.png" alt=""></p>
<p>&#x55EF;&#xFF0C;&#x4F60;&#x61C2;&#x5F97;&#xFF01;</p>
<h2 id="&#x53EA;&#x6B20;&#x4E1C;&#x98CE;">&#x53EA;&#x6B20;&#x4E1C;&#x98CE;</h2>
<p>&#x597D;&#xFF0C;&#x7167;&#x7247;&#x90FD;&#x6709;&#x4E86;&#xFF0C;&#x90A3;&#x4E48;&#x6211;&#x4EEC;&#x5C31;&#x5077;&#x5077;&#x5730;&#x4E0B;&#x8F7D;&#x4E0B;&#x6765;&#x5427;&#xFF5E;</p>
<p>&#x5B8C;&#x5584; domain_page &#x4EE3;&#x7801;&#xFF0C;&#x5B9E;&#x73B0;&#x4FDD;&#x5B58;&#x7B80;&#x4ECB;&#x548C;&#x904D;&#x5386;&#x4FDD;&#x5B58;&#x56FE;&#x7247;&#x7684;&#x65B9;&#x6CD5;&#x3002;</p>
<p>&#x5728;&#x8FD9;&#x91CC;&#xFF0C;PySpider &#x6709;&#x4E00;&#x4E2A;&#x7279;&#x70B9;&#xFF0C;&#x6240;&#x6709;&#x7684; request &#x90FD;&#x4F1A;&#x4FDD;&#x5B58;&#x5230;&#x4E00;&#x4E2A;&#x961F;&#x5217;&#x4E2D;&#xFF0C;&#x5E76;&#x5177;&#x6709;&#x53BB;&#x91CD;&#x548C;&#x81EA;&#x52A8;&#x91CD;&#x8BD5;&#x673A;&#x5236;&#x3002;&#x6240;&#x4EE5;&#xFF0C;&#x6211;&#x4EEC;&#x6700;&#x597D;&#x7684;&#x89E3;&#x51B3;&#x65B9;&#x6CD5;&#x662F;&#xFF0C;&#x628A;&#x6BCF;&#x5F20;&#x56FE;&#x7247;&#x7684;&#x8BF7;&#x6C42;&#x90FD;&#x5199;&#x6210;&#x4E00;&#x4E2A; request&#xFF0C;&#x7136;&#x540E;&#x6210;&#x529F;&#x540E;&#x7528;&#x6587;&#x4EF6;&#x5199;&#x5165;&#x5373;&#x53EF;&#xFF0C;&#x8FD9;&#x6837;&#x4F1A;&#x907F;&#x514D;&#x56FE;&#x7247;&#x52A0;&#x8F7D;&#x4E0D;&#x5168;&#x7684;&#x95EE;&#x9898;&#x3002;</p>
<p>&#x66FE;&#x7ECF;&#x5728;&#x4E4B;&#x524D;&#x6587;&#x7AE0;&#x5199;&#x8FC7;&#x56FE;&#x7247;&#x4E0B;&#x8F7D;&#x548C;&#x6587;&#x4EF6;&#x5939;&#x521B;&#x5EFA;&#x7684;&#x8FC7;&#x7A0B;&#xFF0C;&#x5728;&#x8FD9;&#x91CC;&#x5C31;&#x4E0D;&#x591A;&#x8D58;&#x8FF0;&#x539F;&#x7406;&#x4E86;&#xFF0C;&#x76F4;&#x63A5;&#x4E0A;&#x5199;&#x597D;&#x7684;&#x5DE5;&#x5177;&#x7C7B;&#xFF0C;&#x540E;&#x9762;&#x4F1A;&#x6709;&#x5B8C;&#x6574;&#x4EE3;&#x7801;&#x3002;</p>
<pre><code>import os

class Deal:
    def __init__(self):
        self.path = DIR_PATH
        if not self.path.endswith(&apos;/&apos;):
            self.path = self.path + &apos;/&apos;
        if not os.path.exists(self.path):
            os.makedirs(self.path)

    def mkDir(self, path):
        path = path.strip()
        dir_path = self.path + path
        exists = os.path.exists(dir_path)
        if not exists:
            os.makedirs(dir_path)
            return dir_path
        else:
            return dir_path

    def saveImg(self, content, path):
        f = open(path, &apos;wb&apos;)
        f.write(content)
        f.close()

    def saveBrief(self, content, dir_path, name):
        file_name = dir_path + &quot;/&quot; + name + &quot;.txt&quot;
        f = open(file_name, &quot;w+&quot;)
        f.write(content.encode(&apos;utf-8&apos;))

    def getExtension(self, url):
        extension = url.split(&apos;.&apos;)[-1]
        return extension
</code></pre><p>&#x8FD9;&#x91CC;&#x9762;&#x5305;&#x542B;&#x4E86;&#x56DB;&#x4E2A;&#x65B9;&#x6CD5;&#x3002;</p>
<blockquote>
<p>mkDir&#xFF1A;&#x521B;&#x5EFA;&#x6587;&#x4EF6;&#x5939;&#xFF0C;&#x7528;&#x6765;&#x521B;&#x5EFA; MM &#x540D;&#x5B57;&#x5BF9;&#x5E94;&#x7684;&#x6587;&#x4EF6;&#x5939;&#x3002;
saveBrief: &#x4FDD;&#x5B58;&#x7B80;&#x4ECB;&#xFF0C;&#x4FDD;&#x5B58; MM &#x7684;&#x6587;&#x5B57;&#x7B80;&#x4ECB;&#x3002;
saveImg: &#x4F20;&#x5165;&#x56FE;&#x7247;&#x4E8C;&#x8FDB;&#x5236;&#x6D41;&#x4EE5;&#x53CA;&#x4FDD;&#x5B58;&#x8DEF;&#x5F84;&#xFF0C;&#x5B58;&#x50A8;&#x56FE;&#x7247;&#x3002;
getExtension: &#x83B7;&#x5F97;&#x94FE;&#x63A5;&#x7684;&#x540E;&#x7F00;&#x540D;&#xFF0C;&#x901A;&#x8FC7;&#x56FE;&#x7247; URL &#x83B7;&#x5F97;&#x3002;
&#x7136;&#x540E;&#x5728; domain_page &#x4E2D;&#x5177;&#x4F53;&#x5B9E;&#x73B0;&#x5982;&#x4E0B;</p>
</blockquote>
<pre><code>def domain_page(self, response):
    name = response.doc(&apos;.mm-p-model-info-left-top dd &gt; a&apos;).text()
    dir_path = self.deal.mkDir(name)
    brief = response.doc(&apos;.mm-aixiu-content&apos;).text()
    if dir_path:
        imgs = response.doc(&apos;.mm-aixiu-content img&apos;).items()
        count = 1
        self.deal.saveBrief(brief, dir_path, name)
        for img in imgs:
            url = img.attr.src
            if url:
                extension = self.deal.getExtension(url)
                file_name = name + str(count) + &apos;.&apos; + extension
                count += 1
                self.crawl(img.attr.src, callback=self.save_img,
                           save={&apos;dir_path&apos;: dir_path, &apos;file_name&apos;: file_name})

def save_img(self, response):
    content = response.content
    dir_path = response.save[&apos;dir_path&apos;]
    file_name = response.save[&apos;file_name&apos;]
    file_path = dir_path + &apos;/&apos; + file_name
    self.deal.saveImg(content, file_path)
</code></pre><p>&#x4EE5;&#x4E0A;&#x65B9;&#x6CD5;&#x9996;&#x5148;&#x83B7;&#x53D6;&#x4E86;&#x9875;&#x9762;&#x7684;&#x6240;&#x6709;&#x6587;&#x5B57;&#xFF0C;&#x7136;&#x540E;&#x8C03;&#x7528;&#x4E86; saveBrief &#x65B9;&#x6CD5;&#x5B58;&#x50A8;&#x7B80;&#x4ECB;&#x3002;</p>
<p>&#x7136;&#x540E;&#x904D;&#x5386;&#x4E86; MM &#x6240;&#x6709;&#x7684;&#x56FE;&#x7247;&#xFF0C;&#x5E76;&#x901A;&#x8FC7;&#x94FE;&#x63A5;&#x83B7;&#x53D6;&#x540E;&#x7F00;&#x540D;&#xFF0C;&#x548C; MM &#x7684;&#x59D3;&#x540D;&#x4EE5;&#x53CA;&#x81EA;&#x589E;&#x8BA1;&#x6570;&#x7EC4;&#x5408;&#x6210;&#x4E00;&#x4E2A;&#x65B0;&#x7684;&#x6587;&#x4EF6;&#x540D;&#xFF0C;&#x8C03;&#x7528; saveImg &#x65B9;&#x6CD5;&#x4FDD;&#x5B58;&#x56FE;&#x7247;&#x3002;</p>
<h2 id="&#x7089;&#x706B;&#x7EAF;&#x9752;">&#x7089;&#x706B;&#x7EAF;&#x9752;</h2>
<p>&#x597D;&#xFF0C;&#x57FA;&#x672C;&#x7684;&#x4E1C;&#x897F;&#x90FD;&#x5199;&#x597D;&#x4E86;&#x3002;</p>
<p>&#x63A5;&#x4E0B;&#x6765;&#x3002;&#x7EE7;&#x7EED;&#x5B8C;&#x5584;&#x4E00;&#x4E0B;&#x4EE3;&#x7801;&#x3002;&#x7B2C;&#x4E00;&#x7248;&#x672C;&#x5B8C;&#x6210;&#x3002;</p>
<p>&#x7248;&#x672C;&#x4E00;&#x529F;&#x80FD;&#xFF1A;&#x6309;&#x7167;&#x6DD8;&#x5B9D;MM&#x59D3;&#x540D;&#x5206;&#x6587;&#x4EF6;&#x5939;&#xFF0C;&#x5B58;&#x50A8;MM&#x7684; txt &#x6587;&#x672C;&#x7B80;&#x4ECB;&#x4EE5;&#x53CA;&#x6240;&#x6709;&#x7F8E;&#x56FE;&#x81F3;&#x672C;&#x5730;&#x3002;</p>
<p>&#x53EF;&#x914D;&#x7F6E;&#x9879;&#xFF1A;</p>
<blockquote>
<p>PAGE_START: &#x5217;&#x8868;&#x5F00;&#x59CB;&#x9875;&#x7801;
PAGE_END: &#x5217;&#x8868;&#x7ED3;&#x675F;&#x9875;&#x7801;
DIR_PATH: &#x8D44;&#x6E90;&#x4FDD;&#x5B58;&#x8DEF;&#x5F84;</p>
</blockquote>
<pre><code>#!/usr/bin/env python
# -*- encoding: utf-8 -*-
# Created on 2016-03-25 00:59:45
# Project: taobaomm

from pyspider.libs.base_handler import *

PAGE_START = 1
PAGE_END = 30
DIR_PATH = &apos;/var/py/mm&apos;


class Handler(BaseHandler):
    crawl_config = {
    }

    def __init__(self):
        self.base_url = &apos;https://mm.taobao.com/json/request_top_list.htm?page=&apos;
        self.page_num = PAGE_START
        self.total_num = PAGE_END
        self.deal = Deal()

    def on_start(self):
        while self.page_num &lt;= self.total_num:
            url = self.base_url + str(self.page_num)
            self.crawl(url, callback=self.index_page)
            self.page_num += 1

    def index_page(self, response):
        for each in response.doc(&apos;.lady-name&apos;).items():
            self.crawl(each.attr.href, callback=self.detail_page, fetch_type=&apos;js&apos;)

    def detail_page(self, response):
        domain = response.doc(&apos;.mm-p-domain-info li &gt; span&apos;).text()
        if domain:
            page_url = &apos;https:&apos; + domain
            self.crawl(page_url, callback=self.domain_page)

    def domain_page(self, response):
        name = response.doc(&apos;.mm-p-model-info-left-top dd &gt; a&apos;).text()
        dir_path = self.deal.mkDir(name)
        brief = response.doc(&apos;.mm-aixiu-content&apos;).text()
        if dir_path:
            imgs = response.doc(&apos;.mm-aixiu-content img&apos;).items()
            count = 1
            self.deal.saveBrief(brief, dir_path, name)
            for img in imgs:
                url = img.attr.src
                if url:
                    extension = self.deal.getExtension(url)
                    file_name = name + str(count) + &apos;.&apos; + extension
                    count += 1
                    self.crawl(img.attr.src, callback=self.save_img,
                               save={&apos;dir_path&apos;: dir_path, &apos;file_name&apos;: file_name})

    def save_img(self, response):
        content = response.content
        dir_path = response.save[&apos;dir_path&apos;]
        file_name = response.save[&apos;file_name&apos;]
        file_path = dir_path + &apos;/&apos; + file_name
        self.deal.saveImg(content, file_path)


import os

class Deal:
    def __init__(self):
        self.path = DIR_PATH
        if not self.path.endswith(&apos;/&apos;):
            self.path = self.path + &apos;/&apos;
        if not os.path.exists(self.path):
            os.makedirs(self.path)

    def mkDir(self, path):
        path = path.strip()
        dir_path = self.path + path
        exists = os.path.exists(dir_path)
        if not exists:
            os.makedirs(dir_path)
            return dir_path
        else:
            return dir_path

    def saveImg(self, content, path):
        f = open(path, &apos;wb&apos;)
        f.write(content)
        f.close()

    def saveBrief(self, content, dir_path, name):
        file_name = dir_path + &quot;/&quot; + name + &quot;.txt&quot;
        f = open(file_name, &quot;w+&quot;)
        f.write(content.encode(&apos;utf-8&apos;))

    def getExtension(self, url):
        extension = url.split(&apos;.&apos;)[-1]
        return extension
</code></pre><p>&#x7C98;&#x8D34;&#x5230;&#x4F60;&#x7684; PySpider &#x4E2D;&#x8FD0;&#x884C;&#x5427;&#xFF5E;</p>
<p>&#x5176;&#x4E2D;&#x6709;&#x4E00;&#x4E9B;&#x77E5;&#x8BC6;&#x70B9;&#xFF0C;&#x6211;&#x4F1A;&#x5728;&#x540E;&#x9762;&#x4F5C;&#x8BE6;&#x7EC6;&#x7684;&#x7528;&#x6CD5;&#x603B;&#x7ED3;&#x3002;&#x5927;&#x5BB6;&#x53EF;&#x4EE5;&#x5148;&#x4F53;&#x4F1A;&#x4E00;&#x4E0B;&#x4EE3;&#x7801;&#x3002;</p>
<p><img src="../image/chapter4/section4-13.png" alt=""></p>
<p>&#x4FDD;&#x5B58;&#x4E4B;&#x540E;&#xFF0C;&#x70B9;&#x51FB;&#x4E0B;&#x65B9;&#x7684; run&#xFF0C;&#x4F60;&#x4F1A;&#x53D1;&#x73B0;&#xFF0C;&#x6D77;&#x91CF;&#x7684; MM &#x56FE;&#x7247;&#x5DF2;&#x7ECF;&#x6D8C;&#x5165;&#x4F60;&#x7684;&#x7535;&#x8111;&#x5566;&#xFF5E;</p>
<p><img src="../image/chapter4/section4-14.png" alt=""></p>
<p><img src="../image/chapter4/section4-15.png" alt=""></p>
<p>&#x9700;&#x8981;&#x89E3;&#x91CA;&#xFF1F;&#x9700;&#x8981;&#x6211;&#x4E5F;&#x4E0D;&#x89E3;&#x91CA;&#xFF01;</p>
<h2 id="&#x9879;&#x76EE;&#x4EE3;&#x7801;">&#x9879;&#x76EE;&#x4EE3;&#x7801;</h2>
<p><a href="https://github.com/cqcre/TaobaoMM" target="_blank">TaobaoMM &#x2013; GitHub</a></p>
<h2 id="&#x5C1A;&#x65B9;&#x5B9D;&#x5251;">&#x5C1A;&#x65B9;&#x5B9D;&#x5251;</h2>
<p>&#x5982;&#x679C;&#x60F3;&#x4E86;&#x89E3; PySpider &#x7684;&#x66F4;&#x591A;&#x5185;&#x5BB9;&#xFF0C;&#x53EF;&#x4EE5;&#x67E5;&#x770B;<a href="http://docs.pyspider.org/en/latest/Quickstart/" target="_blank">&#x5B98;&#x65B9;&#x6587;&#x6863;</a>&#x3002;</p>

                    
                    </section>
                
                
                </div>
            </div>
        </div>

        
        <a href="../chapter4/section3.html" class="navigation navigation-prev " aria-label="Previous page: 3. Python爬虫进阶三之爬虫框架Scrapy安装配置"><i class="fa fa-angle-left"></i></a>
        
        
        <a href="../chapter4/section5.html" class="navigation navigation-next " aria-label="Next page: 5. Python爬虫进阶五之多线程的用法"><i class="fa fa-angle-right"></i></a>
        
    </div>
</div>

        
<script src="../gitbook/app.js"></script>

    
    <script src="../gitbook/plugins/gitbook-plugin-search/lunr.min.js"></script>
    

    
    <script src="../gitbook/plugins/gitbook-plugin-search/search.js"></script>
    

    
    <script src="../gitbook/plugins/gitbook-plugin-sharing/buttons.js"></script>
    

    
    <script src="../gitbook/plugins/gitbook-plugin-fontsettings/buttons.js"></script>
    

    
    <script src="../gitbook/plugins/gitbook-plugin-livereload/plugin.js"></script>
    

<script>
require(["gitbook"], function(gitbook) {
    var config = {"highlight":{},"search":{"maxIndexSize":1000000},"sharing":{"facebook":true,"twitter":true,"google":false,"weibo":false,"instapaper":false,"vk":false,"all":["facebook","google","twitter","weibo","instapaper"]},"fontsettings":{"theme":"white","family":"sans","size":2},"livereload":{}};
    gitbook.start(config);
});
</script>

        
    </body>
    
</html>
